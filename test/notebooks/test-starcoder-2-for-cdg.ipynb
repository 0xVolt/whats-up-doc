{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Run Inference on StarCoder2-7B","metadata":{}},{"cell_type":"markdown","source":"## Install and Load Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install accelerate","metadata":{"execution":{"iopub.status.busy":"2024-06-11T11:31:18.631350Z","iopub.execute_input":"2024-06-11T11:31:18.631724Z","iopub.status.idle":"2024-06-11T11:31:31.525269Z","shell.execute_reply.started":"2024-06-11T11:31:18.631695Z","shell.execute_reply":"2024-06-11T11:31:31.524287Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom datetime import datetime\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-11T11:31:31.527344Z","iopub.execute_input":"2024-06-11T11:31:31.527664Z","iopub.status.idle":"2024-06-11T11:31:31.532368Z","shell.execute_reply.started":"2024-06-11T11:31:31.527634Z","shell.execute_reply":"2024-06-11T11:31:31.531379Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Import Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"checkpoint = \"bigcode/starcoder2-7b\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# for fp16 use `torch_dtype=torch.float16` instead\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", torch_dtype=torch.float16)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T11:31:31.533448Z","iopub.execute_input":"2024-06-11T11:31:31.533702Z","iopub.status.idle":"2024-06-11T11:31:44.260152Z","shell.execute_reply.started":"2024-06-11T11:31:31.533680Z","shell.execute_reply":"2024-06-11T11:31:44.259386Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38fa931d3897400ebc9b19a2d8a65c53"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Create Test Prompt","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"\nWrite a script in Python that implements a circular queue and creates one with user input. Give an example of how the script works and document the code with appropriate comments and docstrings wherever necessary. \nGenerate a response only for the prompt that is given and nothing else. \n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-11T11:44:25.466191Z","iopub.execute_input":"2024-06-11T11:44:25.466575Z","iopub.status.idle":"2024-06-11T11:44:25.471008Z","shell.execute_reply.started":"2024-06-11T11:44:25.466549Z","shell.execute_reply":"2024-06-11T11:44:25.470067Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference with `model.generate()`","metadata":{}},{"cell_type":"code","source":"%%time\ninputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs, max_new_tokens=256)\nprint(tokenizer.decode(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2024-06-11T11:32:21.781684Z","iopub.execute_input":"2024-06-11T11:32:21.782068Z","iopub.status.idle":"2024-06-11T11:32:37.837203Z","shell.execute_reply.started":"2024-06-11T11:32:21.782040Z","shell.execute_reply":"2024-06-11T11:32:37.836279Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nQuestion: Write a script in Python that implements a circular queue and creates one with user input.\nAnswer:\n\n```\nclass CircularQueue:\n    def __init__(self, size):\n        self.size = size\n        self.queue = [None] * size\n        self.front = 0\n        self.rear = 0\n\n    def enqueue(self, data):\n        if self.is_full():\n            print(\"Queue is full\")\n            return\n        self.queue[self.rear] = data\n        self.rear = (self.rear + 1) % self.size\n\n    def dequeue(self):\n        if self.is_empty():\n            print(\"Queue is empty\")\n            return\n        data = self.queue[self.front]\n        self.front = (self.front + 1) % self.size\n        return data\n\n    def is_empty(self):\n        return self.front == self.rear\n\n    def is_full(self):\n        return (self.rear + 1) % self.size == self.front\n\n    def print_queue(self):\n        if self.is_empty():\n            print(\"Queue is empty\")\n            return\n        print(\"Queue:\")\n        for i in range(self.front, self.rear):\n            print(self.queue[i], end=\" \")\n        print()\nCPU times: user 16 s, sys: 0 ns, total: 16 s\nWall time: 16.1 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Run Inference with Pipeline","metadata":{}},{"cell_type":"code","source":"def generateOutputWithModelPipeline(model, tokenizer, prompt, temperature=0.7, max_new_tokens=256) : \n    pipe = pipeline(\n        task=\"text-generation\", \n        model=model, \n        tokenizer=tokenizer\n    )\n    \n    start = datetime.now()\n    \n    output = pipe(\n        prompt,\n        do_sample=True,\n        max_new_tokens=max_new_tokens, \n        temperature=temperature, \n        top_k=50, \n        top_p=0.95,\n        num_return_sequences=1\n    )\n    \n    stop = datetime.now()\n    \n    totalTimeToPrompt = stop - start\n    print(f\"Execution Time : {totalTimeToPrompt}\")\n    \n    return output","metadata":{"execution":{"iopub.status.busy":"2024-06-11T11:44:29.388365Z","iopub.execute_input":"2024-06-11T11:44:29.388731Z","iopub.status.idle":"2024-06-11T11:44:29.395337Z","shell.execute_reply.started":"2024-06-11T11:44:29.388700Z","shell.execute_reply":"2024-06-11T11:44:29.394307Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"output = generateOutputWithModelPipeline(model, tokenizer, prompt, max_new_tokens=1024)\nprint(output[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-06-11T11:44:35.863901Z","iopub.execute_input":"2024-06-11T11:44:35.864981Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}