{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install accelerate","metadata":{"execution":{"iopub.status.busy":"2024-06-11T11:31:18.631350Z","iopub.execute_input":"2024-06-11T11:31:18.631724Z","iopub.status.idle":"2024-06-11T11:31:31.525269Z","shell.execute_reply.started":"2024-06-11T11:31:18.631695Z","shell.execute_reply":"2024-06-11T11:31:31.524287Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom datetime import datetime\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-11T11:31:31.527344Z","iopub.execute_input":"2024-06-11T11:31:31.527664Z","iopub.status.idle":"2024-06-11T11:31:31.532368Z","shell.execute_reply.started":"2024-06-11T11:31:31.527634Z","shell.execute_reply":"2024-06-11T11:31:31.531379Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"checkpoint = \"bigcode/starcoder2-7b\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# for fp16 use `torch_dtype=torch.float16` instead\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", torch_dtype=torch.float16)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T11:31:31.533448Z","iopub.execute_input":"2024-06-11T11:31:31.533702Z","iopub.status.idle":"2024-06-11T11:31:44.260152Z","shell.execute_reply.started":"2024-06-11T11:31:31.533680Z","shell.execute_reply":"2024-06-11T11:31:44.259386Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38fa931d3897400ebc9b19a2d8a65c53"}},"metadata":{}}]},{"cell_type":"code","source":"def generateOutputWithModelPipeline(model, tokenizer, prompt, temperature=0.7) : \n    pipe = pipeline(\n        task=\"text-generation\", \n        model=model, \n        tokenizer=tokenizer\n    )\n    \n    start = datetime.now()\n    \n    output = pipe(\n        prompt,\n        do_sample=True,\n        max_new_tokens=256, \n        temperature=temperature, \n        top_k=50, \n        top_p=0.95,\n        num_return_sequences=1\n    )\n    \n    stop = datetime.now()\n    \n    totalTimeToPrompt = stop - start\n    print(f\"Execution Time : {totalTimeToPrompt}\")\n    \n    return output","metadata":{"execution":{"iopub.status.busy":"2024-06-11T11:31:44.261901Z","iopub.execute_input":"2024-06-11T11:31:44.262185Z","iopub.status.idle":"2024-06-11T11:31:44.268130Z","shell.execute_reply.started":"2024-06-11T11:31:44.262161Z","shell.execute_reply":"2024-06-11T11:31:44.267191Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"\nQuestion: Write a script in Python that implements a circular queue and creates one with user input.\nAnswer:\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-11T11:31:44.269354Z","iopub.execute_input":"2024-06-11T11:31:44.269665Z","iopub.status.idle":"2024-06-11T11:31:44.277844Z","shell.execute_reply.started":"2024-06-11T11:31:44.269637Z","shell.execute_reply":"2024-06-11T11:31:44.276986Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"generateOutputWithModelPipeline(model, tokenizer, prompt)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T11:31:44.278873Z","iopub.execute_input":"2024-06-11T11:31:44.279187Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\ninputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}