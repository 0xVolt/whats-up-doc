{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0xVolt/whats-up-doc/blob/main/src/experimental-notebooks/compare-models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UED-fV4iG5iu"
      },
      "source": [
        "# Compare Various HuggingFace Models for CDG by `SmoothedBLEU` Score\n",
        "\n",
        "## Purpose\n",
        "\n",
        "The purpose of this notebook is to figure out which models will work best for our CLI app. We will calculate and compare these select models by their smoothed BLEU scores and possibly look into fine-tuning them with [this dataset](https://www.dropbox.com/sh/488bq2of10r4wvw/AACs5CGIQuwtsD7j_Ls_JAORa/finetuning_dataset?dl=0&subfolder_nav_tracking=1).\n",
        "\n",
        "## Models\n",
        "\n",
        "These are the models we'll create inference points for and load in this notebook.\n",
        "1. [SEBIS/code_trans_t5_base_code_documentation_generation_python](https://huggingface.co/SEBIS/code_trans_t5_base_code_documentation_generation_python)\n",
        "2. [Salesforce/codet5-base-multi-sum](https://huggingface.co/Salesforce/codet5-base-multi-sum)\n",
        "3. [google/flan-t5-small](https://huggingface.co/google/flan-t5-small)\n",
        "\n",
        "The models listed above have been fine-tuned on the CodeSearchNet dataset across various programming languages for PL-NL sequence-to-sequence tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8E79z95HYnR"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wJbpYXdGluQ",
        "outputId": "9f9a1b40-ff83-4431-98d3-05fbc0927d70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q transformers sentencepiece datasets nltk accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\deshi\\anaconda3\\envs\\py39-torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelWithLMHead, RobertaTokenizer, T5Tokenizer, T5ForConditionalGeneration, SummarizationPipeline\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from datasets import load_dataset\n",
        "import tokenize\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Evaluation Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KjjG2VfSHXpA"
      },
      "outputs": [],
      "source": [
        "def calculateSmoothedBLEU(reference, candidate):\n",
        "    \"\"\"\n",
        "    Calculate the Smoothed BLEU-4 score between a reference string and a candidate string.\n",
        "\n",
        "    Args:\n",
        "    reference (str): The reference (ground truth) string.\n",
        "    candidate (str): The candidate (generated) string.\n",
        "\n",
        "    Returns:\n",
        "    float: The Smoothed BLEU-4 score.\n",
        "    \"\"\"\n",
        "    reference_tokens = reference.split()\n",
        "    candidate_tokens = candidate.split()\n",
        "\n",
        "    smoother = SmoothingFunction()\n",
        "    bleu_score = sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoother.method1)\n",
        "\n",
        "    return bleu_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Python Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pythonTokenizer(line):\n",
        "    result = []\n",
        "    line = io.StringIO(line)\n",
        "\n",
        "    for tokenType, token, start, end, line in tokenize.generate_tokens(line.readline):\n",
        "        if (not tokenType == tokenize.COMMENT):\n",
        "            if tokenType == tokenize.STRING:\n",
        "                result.append(\"CODE_STRING\")\n",
        "            elif tokenType == tokenize.NUMBER:\n",
        "                result.append(\"CODE_INTEGER\")\n",
        "            elif (not token == \"\\n\") and (not token == \"    \"):\n",
        "                result.append(str(token))\n",
        "                \n",
        "    return ' '.join(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CodeTransT5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\deshi\\anaconda3\\envs\\py39-torch\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1509: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ],
      "source": [
        "codeTransModel = AutoModelWithLMHead.from_pretrained(\"SEBIS/code_trans_t5_base_source_code_summarization_python_transfer_learning_finetune\")\n",
        "codeTransTokenizer = AutoTokenizer.from_pretrained(\"SEBIS/code_trans_t5_base_source_code_summarization_python_transfer_learning_finetune\", skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CodeT5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "codeT5Model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base-multi-sum')\n",
        "codeT5Tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base-multi-sum', skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FlanT5 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading model.safetensors: 100%|██████████| 308M/308M [03:28<00:00, 1.47MB/s] \n",
            "c:\\Users\\deshi\\anaconda3\\envs\\py39-torch\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\deshi\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 72.9kB/s]\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 452kB/s]\n",
            "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 1.14MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 210kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:01<00:00, 1.21MB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "flanT5Model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", device_map=\"auto\")\n",
        "flanT5Tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\", skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "codeTransPipeline = SummarizationPipeline(\n",
        "    model=codeTransModel,\n",
        "    tokenizer=codeTransTokenizer,\n",
        "    device=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "codeT5Pipeline = SummarizationPipeline(\n",
        "    model=codeT5Model,\n",
        "    tokenizer=codeT5Tokenizer,\n",
        "    device=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "flanPipeline = SummarizationPipeline(\n",
        "    model=flanT5Model,\n",
        "    tokenizer=flanT5Tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "code = '''\n",
        "\n",
        "def is_prime(number):\n",
        "    if number <= 1:\n",
        "        return False\n",
        "    elif number <= 3:\n",
        "        return True\n",
        "    elif number % 2 == 0 or number % 3 == 0:\n",
        "        return False\n",
        "    i = 5\n",
        "    while i * i <= number:\n",
        "        if number % i == 0 or number % (i + 2) == 0:\n",
        "            return False\n",
        "        i += 6\n",
        "    return True\n",
        "        \n",
        "''' #@param {type:\"raw\"}\n",
        "\n",
        "tokenizedCode = pythonTokenizer(code)\n",
        "label = \"A code block in Python to check whether a number is prime or not.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\deshi\\anaconda3\\envs\\py39-torch\\lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "codeTransOutput = codeTransPipeline([tokenizedCode])[0]['summary_text']\n",
        "codeT5Output = codeT5Pipeline([tokenizedCode])[0]['summary_text']\n",
        "flanOutput = flanPipeline([tokenizedCode])[0]['summary_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Dictionary of Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.04535390934416274"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calculateSmoothedBLEU(codeTransOutput, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = {\n",
        "    'codeTrans': {\n",
        "        'output': codeTransOutput,\n",
        "        'score': calculateSmoothedBLEU(codeTransOutput, label)\n",
        "    },\n",
        "    'codeT5': {\n",
        "        'output': codeT5Output,\n",
        "        'score': calculateSmoothedBLEU(codeT5Output, label)\n",
        "    },\n",
        "    'flanT5': {\n",
        "        'output': flanOutput,\n",
        "        'score': calculateSmoothedBLEU(flanOutput, label)\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'codeTrans': {'output': \"What 's the most efficient way to check if an integer is prime in Python ?\",\n",
              "  'score': 0.04535390934416274},\n",
              " 'codeT5': {'output': 'Check if number is prime .',\n",
              "  'score': 0.07069301148938888},\n",
              " 'flanT5': {'output': 'True', 'score': 0}}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPTWE3sLB2iBAUp0BOuv8y3",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
