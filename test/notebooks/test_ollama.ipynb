{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Ollama with LangChain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the downloaded model from Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 250 ms\n",
      "Wall time: 249 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "llm = Ollama(model='codellama')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 93.8 ms\n",
      "Wall time: 8.11 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nHello! I'm doing well, thank you for asking. It's nice to connect with you as well. Is there something on your mind that you'd like to talk about or ask?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llmReturn = llm.invoke(\"Hi! How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello! I'm doing well, thank you for asking. It's nice to connect with you as well. Is there something on your mind that you'd like to talk about or ask?\n"
     ]
    }
   ],
   "source": [
    "print(llmReturn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming the LLM Response Word by Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You\n",
      " can\n",
      " use\n",
      " this\n",
      " format\n",
      ":\n",
      "\n",
      "\n",
      "*\n",
      " An\n",
      " L\n",
      "LM\n",
      ",\n",
      " or\n",
      " Master\n",
      " of\n",
      " La\n",
      "ws\n",
      ",\n",
      " degree\n",
      " is\n",
      " a\n",
      " two\n",
      "-\n",
      "year\n",
      " post\n",
      "grad\n",
      "uate\n",
      " degree\n",
      " that\n",
      " allows\n",
      " students\n",
      " to\n",
      " gain\n",
      " special\n",
      "ized\n",
      " knowledge\n",
      " in\n",
      " a\n",
      " specific\n",
      " legal\n",
      " area\n",
      ".\n",
      "\n",
      "\n",
      "*\n",
      " It\n",
      "'\n",
      "s\n",
      " designed\n",
      " for\n",
      " those\n",
      " who\n",
      " want\n",
      " to\n",
      " improve\n",
      " their\n",
      " expert\n",
      "ise\n",
      " in\n",
      " a\n",
      " particular\n",
      " subject\n",
      ",\n",
      " such\n",
      " as\n",
      " corpor\n",
      "ate\n",
      " law\n",
      ",\n",
      " intellectual\n",
      " property\n",
      " law\n",
      ",\n",
      " or\n",
      " international\n",
      " trade\n",
      " law\n",
      ".\n",
      "\n",
      "\n",
      "*\n",
      " An\n",
      " L\n",
      "LM\n",
      " degree\n",
      " requires\n",
      " passing\n",
      " an\n",
      " entrance\n",
      " exam\n",
      " and\n",
      " comple\n",
      "ting\n",
      " course\n",
      "work\n",
      " with\n",
      " a\n",
      " th\n",
      "esis\n",
      " or\n",
      " research\n",
      " project\n",
      ".\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is an LLM in 3 bullet points?\"\n",
    "\n",
    "for chunks in llm.stream(query):\n",
    "    print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A better stream format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stream = llm.stream(\"\"\"\n",
    "    What is a Large Language Model in 3 bullet points?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Large Language Models are neural networks that are trained on vast amounts of text data to generate language outputs that are coherent and natural-sounding.\n",
      "  2. The key to the success of large language models is their ability to learn patterns in large datasets, which allows them to generalize well to new, unseen texts.\n",
      "  3. Large Language Models have been used for a variety of applications, including language translation, text summarization, and chatbots, among others. They are also being explored as a potential solution to the \"Grand Challenges\" of Artificial Intelligence, such as language generation and comprehension.CPU times: total: 93.8 ms\n",
      "Wall time: 25.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for chunk in stream:\n",
    "    print(chunk, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whats-up-doc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
