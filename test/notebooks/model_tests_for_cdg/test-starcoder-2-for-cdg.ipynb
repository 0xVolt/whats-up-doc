{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Run Inference on StarCoder2-7B","metadata":{}},{"cell_type":"markdown","source":"## References\n1. [StarCoder2-7B's HuggingFace Repo](https://huggingface.co/bigcode/starcoder2-7b)","metadata":{}},{"cell_type":"markdown","source":"## Install and Load Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install accelerate","metadata":{"execution":{"iopub.status.busy":"2024-06-11T14:18:34.631972Z","iopub.execute_input":"2024-06-11T14:18:34.632324Z","iopub.status.idle":"2024-06-11T14:18:48.395032Z","shell.execute_reply.started":"2024-06-11T14:18:34.632296Z","shell.execute_reply":"2024-06-11T14:18:48.394115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom datetime import datetime\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-11T14:18:48.397211Z","iopub.execute_input":"2024-06-11T14:18:48.397908Z","iopub.status.idle":"2024-06-11T14:18:58.910365Z","shell.execute_reply.started":"2024-06-11T14:18:48.397868Z","shell.execute_reply":"2024-06-11T14:18:58.909595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"checkpoint = \"bigcode/starcoder2-7b\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# for fp16 use `torch_dtype=torch.float16` instead\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", torch_dtype=torch.float16)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T14:18:58.911434Z","iopub.execute_input":"2024-06-11T14:18:58.911979Z","iopub.status.idle":"2024-06-11T14:19:30.844450Z","shell.execute_reply.started":"2024-06-11T14:18:58.911952Z","shell.execute_reply":"2024-06-11T14:19:30.843612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")","metadata":{"execution":{"iopub.status.busy":"2024-06-11T14:19:30.846425Z","iopub.execute_input":"2024-06-11T14:19:30.846777Z","iopub.status.idle":"2024-06-11T14:19:30.855277Z","shell.execute_reply.started":"2024-06-11T14:19:30.846750Z","shell.execute_reply":"2024-06-11T14:19:30.854312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Test Prompt","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"\nWrite a script in Python that implements a circular queue and creates one with user input. Give an example of how the script works and document the code with appropriate comments and docstrings wherever necessary. \nGenerate a response only for the prompt that is given and nothing else. \n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-11T14:19:30.856332Z","iopub.execute_input":"2024-06-11T14:19:30.856592Z","iopub.status.idle":"2024-06-11T14:19:30.865840Z","shell.execute_reply.started":"2024-06-11T14:19:30.856570Z","shell.execute_reply":"2024-06-11T14:19:30.864965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference with `model.generate()`","metadata":{}},{"cell_type":"code","source":"%%time\ninputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs, max_new_tokens=256)\nprint(tokenizer.decode(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2024-06-11T14:19:30.866947Z","iopub.execute_input":"2024-06-11T14:19:30.867299Z","iopub.status.idle":"2024-06-11T14:19:47.565030Z","shell.execute_reply.started":"2024-06-11T14:19:30.867256Z","shell.execute_reply":"2024-06-11T14:19:47.563989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference with Pipeline","metadata":{}},{"cell_type":"code","source":"def generateOutputWithModelPipeline(model, tokenizer, prompt, temperature=0.7, max_new_tokens=256) : \n    textGenerator = pipeline(\n        task=\"text-generation\", \n        model=model, \n        tokenizer=tokenizer,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    \n    start = datetime.now()\n    \n    output = textGenerator(\n        prompt,\n        do_sample=True,\n        max_new_tokens=max_new_tokens, \n        temperature=temperature, \n        top_k=50, \n        top_p=0.9,\n        num_return_sequences=1\n    )\n    \n    stop = datetime.now()\n    \n    totalTimeToPrompt = stop - start\n    print(f\"Execution Time : {totalTimeToPrompt}\")\n    \n    return output","metadata":{"execution":{"iopub.status.busy":"2024-06-11T14:19:47.566341Z","iopub.execute_input":"2024-06-11T14:19:47.566669Z","iopub.status.idle":"2024-06-11T14:19:47.573285Z","shell.execute_reply.started":"2024-06-11T14:19:47.566641Z","shell.execute_reply":"2024-06-11T14:19:47.572349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = generateOutputWithModelPipeline(model, tokenizer, prompt)\nprint(output[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-06-11T14:19:47.574516Z","iopub.execute_input":"2024-06-11T14:19:47.574884Z","iopub.status.idle":"2024-06-11T14:20:03.887520Z","shell.execute_reply.started":"2024-06-11T14:19:47.574854Z","shell.execute_reply":"2024-06-11T14:20:03.886618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Pipeline with Custom Input","metadata":{}},{"cell_type":"code","source":"language = \"Python\"\ncode = \"\"\"\ndef test_euler_method(self):\n    def dydx(x, y):\n        return x + y\n\n    x0, y0 = 0, 1\n    x_end = 1\n    h = 0.01\n    x, y = differential_equations.euler_method(dydx, x0, y0, x_end, h)\n    expected = np.exp(x_end) - x_end - 1  # Analytical solution for the differential equation dy/dx = x + y\n    self.assertAlmostEqual(y[-1], expected, places=2)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-11T14:20:03.888929Z","iopub.execute_input":"2024-06-11T14:20:03.889451Z","iopub.status.idle":"2024-06-11T14:20:03.894493Z","shell.execute_reply.started":"2024-06-11T14:20:03.889264Z","shell.execute_reply":"2024-06-11T14:20:03.893448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template = f\"\"\"\nGiven a script file in {language}, generate its documentation for each function. For each function in the script, document its name, arguments, return values, and a brief explanation of its logic. Ensure that code within comments is not parsed and documented. Generate nothing else than what is asked.\n\nStrictly use the following format for each function:\n\n## Function Name: `function_name`\n\n### Arguments\n* `arg1` (type): Description of argument 1.\n* `arg2` (type): Description of argument 2.\n* ...\n\n### Return Values\n* `return_value1` (type): Description of return value 1.\n* `return_value2` (type): Description of return value 2.\n* ...\n\n### Explanation of Function Logic:\n1. Brief explanation of the function logic step by step.\n2. ...\n3. ...\n\n-----------------------------------------------------------------------------\n\nHere is the function I want you to generate documentation of:\n\n{code}\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-11T14:20:03.897088Z","iopub.execute_input":"2024-06-11T14:20:03.897354Z","iopub.status.idle":"2024-06-11T14:20:03.916708Z","shell.execute_reply.started":"2024-06-11T14:20:03.897331Z","shell.execute_reply":"2024-06-11T14:20:03.915798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template","metadata":{"execution":{"iopub.status.busy":"2024-06-11T14:20:03.917792Z","iopub.execute_input":"2024-06-11T14:20:03.918067Z","iopub.status.idle":"2024-06-11T14:20:03.929193Z","shell.execute_reply.started":"2024-06-11T14:20:03.918043Z","shell.execute_reply":"2024-06-11T14:20:03.928253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = generateOutputWithModelPipeline(model, tokenizer, template)\ngeneratedText = output[0]['generated_text']\nresponse = generatedText[len(template):].strip()","metadata":{"execution":{"iopub.status.busy":"2024-06-11T14:20:03.930129Z","iopub.execute_input":"2024-06-11T14:20:03.930376Z","iopub.status.idle":"2024-06-11T14:20:20.335245Z","shell.execute_reply.started":"2024-06-11T14:20:03.930354Z","shell.execute_reply":"2024-06-11T14:20:20.334295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(generatedText)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T14:20:20.336349Z","iopub.execute_input":"2024-06-11T14:20:20.336649Z","iopub.status.idle":"2024-06-11T14:20:20.341439Z","shell.execute_reply.started":"2024-06-11T14:20:20.336623Z","shell.execute_reply":"2024-06-11T14:20:20.340517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(response)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T14:20:20.342451Z","iopub.execute_input":"2024-06-11T14:20:20.342711Z","iopub.status.idle":"2024-06-11T14:20:20.353841Z","shell.execute_reply.started":"2024-06-11T14:20:20.342689Z","shell.execute_reply":"2024-06-11T14:20:20.352790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remarks\n\nI'm sure StarCoder2 is great in it's own regard as a code completion agent, however, for my use case, it simply won't cut it.","metadata":{}}]}