{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!mamba install --quiet --force-reinstall aiohttp -y\n!pip install -qU \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n!pip install -q \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n# !pip install wandb evaluate accelerate\n\n# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0 pprint","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-23T14:52:30.319988Z","iopub.execute_input":"2024-07-23T14:52:30.320613Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/c6f2354e.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/86b0f08d.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/c9ddbd6b.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/b121c3e7.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/497deca9.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/09cdf8bf.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/47929eba.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/3e39a7aa.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/2ce54b42.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/4ea078d6.json\" was modified by another program\n","output_type":"stream"}]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nimport pprint\nfrom datasets import load_dataset\nimport torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq_length = 2048\ndtype = None\nload_in_4bit = True\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 32,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\", # Set to True if out of memory\n    random_state = 3407,\n    use_rslora = False,\n    loftq_config = None,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpacaPrompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-02T12:47:08.933386Z","iopub.status.idle":"2024-07-02T12:47:08.933838Z","shell.execute_reply.started":"2024-07-02T12:47:08.933636Z","shell.execute_reply":"2024-07-02T12:47:08.933655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EOS Token is required to stop open-ended generation and eventual hallucination\nEOS_TOKEN = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-07-02T12:47:08.935473Z","iopub.status.idle":"2024-07-02T12:47:08.935852Z","shell.execute_reply.started":"2024-07-02T12:47:08.935672Z","shell.execute_reply":"2024-07-02T12:47:08.935687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def promptFormattingFunction(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = alpacaPrompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass","metadata":{"execution":{"iopub.status.busy":"2024-07-02T12:47:08.939140Z","iopub.status.idle":"2024-07-02T12:47:08.940036Z","shell.execute_reply.started":"2024-07-02T12:47:08.939712Z","shell.execute_reply":"2024-07-02T12:47:08.939740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"code-search-net/code_search_net\", split=\"train[:1000]\")\ndataset = dataset.map(promptFormattingFunction, batched=True,)\n\n# Lowering test dataset size to train faster\ndatasetDictionary = dataset.train_test_split(test_size=0.004)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T12:47:08.941574Z","iopub.status.idle":"2024-07-02T12:47:08.942202Z","shell.execute_reply.started":"2024-07-02T12:47:08.941867Z","shell.execute_reply":"2024-07-02T12:47:08.941891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments(\n    per_device_train_batch_size = 2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps = 4,\n    evaluation_strategy=\"steps\",\n    warmup_ratio = 0.1,\n    num_train_epochs = 1,\n    learning_rate = 2e-5,\n    fp16 = not torch.cuda.is_bf16_supported(),\n    bf16 = torch.cuda.is_bf16_supported(),\n    optim = \"adamw_8bit\",\n    weight_decay = 0.1,\n    lr_scheduler_type = \"linear\",\n    seed = 3407,\n    output_dir = \"outputs\",\n    logging_steps = 1,\n    logging_strategy = 'steps',\n    save_total_limit = 2,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:40:14.079634Z","iopub.status.idle":"2024-07-02T09:40:14.079992Z","shell.execute_reply.started":"2024-07-02T09:40:14.079802Z","shell.execute_reply":"2024-07-02T09:40:14.079816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset_dict[\"train\"],\n    eval_dataset = dataset_dict[\"test\"],\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = True, # Packs short sequences together to save time!\n    args = args,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:40:14.080794Z","iopub.status.idle":"2024-07-02T09:40:14.081129Z","shell.execute_reply.started":"2024-07-02T09:40:14.080970Z","shell.execute_reply":"2024-07-02T09:40:14.080984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:40:14.082591Z","iopub.status.idle":"2024-07-02T09:40:14.082912Z","shell.execute_reply.started":"2024-07-02T09:40:14.082756Z","shell.execute_reply":"2024-07-02T09:40:14.082769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Continue the fibonnaci sequence.\", # instruction\n        \"1, 1, 2, 3, 5, 8\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)","metadata":{},"execution_count":null,"outputs":[]}]}