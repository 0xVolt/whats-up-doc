{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!mamba install --quiet --force-reinstall aiohttp -y\n!pip install -qU \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n!pip install -q \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n# !pip install wandb evaluate accelerate\n\n# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0 pprint","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-23T14:52:30.319988Z","iopub.execute_input":"2024-07-23T14:52:30.320613Z","iopub.status.idle":"2024-07-23T14:56:53.258463Z","shell.execute_reply.started":"2024-07-23T14:52:30.320580Z","shell.execute_reply":"2024-07-23T14:56:53.256932Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/c6f2354e.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/86b0f08d.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/c9ddbd6b.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/b121c3e7.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/497deca9.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/09cdf8bf.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/47929eba.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/3e39a7aa.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/2ce54b42.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/4ea078d6.json\" was modified by another program\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\n\u001b[33mWARNING: huggingface-hub 0.23.2 does not provide the extra 'hf-transfer'\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting datasets==2.16.0\n  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\nCollecting fsspec==2023.10.0\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nCollecting gcsfs==2023.10.0\n  Downloading gcsfs-2023.10.0-py2.py3-none-any.whl.metadata (1.6 kB)\n\u001b[31mERROR: Could not find a version that satisfies the requirement pprint (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for pprint\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nimport pprint\nfrom datasets import load_dataset\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-07-23T14:56:53.264019Z","iopub.execute_input":"2024-07-23T14:56:53.264350Z","iopub.status.idle":"2024-07-23T14:57:10.218134Z","shell.execute_reply.started":"2024-07-23T14:56:53.264312Z","shell.execute_reply":"2024-07-23T14:57:10.217356Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2024-07-23 14:57:01.554465: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-23 14:57:01.554563: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-23 14:57:01.641000: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"max_seq_length = 2048\ndtype = None\nload_in_4bit = True\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T14:57:10.219347Z","iopub.execute_input":"2024-07-23T14:57:10.220415Z","iopub.status.idle":"2024-07-23T14:57:33.124713Z","shell.execute_reply.started":"2024-07-23T14:57:10.220379Z","shell.execute_reply":"2024-07-23T14:57:33.123875Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a47881444f4f4bf9b778b2ea75f8df47"}},"metadata":{}},{"name":"stdout","text":"==((====))==  Unsloth: Fast Llama patching release 2024.7\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.25.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46b9fd1010a441f49e204dd08865ccb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/131 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb892f348b014fda8c37a463496b6e45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7471c0aa588f46fcaa9ec3f445b4e63f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db1f52aaf49847f2b1f1d4ea03de610c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d697d0962e74999bed95b4f2dc04ba8"}},"metadata":{}}]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 32,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\", # Set to True if out of memory\n    random_state = 3407,\n    use_rslora = False,\n    loftq_config = None,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T14:57:33.126482Z","iopub.execute_input":"2024-07-23T14:57:33.126773Z","iopub.status.idle":"2024-07-23T14:57:38.229842Z","shell.execute_reply.started":"2024-07-23T14:57:33.126749Z","shell.execute_reply":"2024-07-23T14:57:38.228813Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}]},{"cell_type":"code","source":"def formatFunction(sample):\n    language = sample['language']\n    instruction = f\"What does this {language} function do?\"\n    inputText = sample['func_code_string']\n    outputText = sample['func_documentation_string']\n\n    return {\n        \"instruction\": instruction,\n        \"input\": inputText,\n        \"output\": outputText\n    }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpacaPrompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-02T12:47:08.933386Z","iopub.status.idle":"2024-07-02T12:47:08.933838Z","shell.execute_reply.started":"2024-07-02T12:47:08.933636Z","shell.execute_reply":"2024-07-02T12:47:08.933655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EOS Token is required to stop open-ended generation and eventual hallucination\nEOS_TOKEN = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-07-02T12:47:08.935473Z","iopub.status.idle":"2024-07-02T12:47:08.935852Z","shell.execute_reply.started":"2024-07-02T12:47:08.935672Z","shell.execute_reply":"2024-07-02T12:47:08.935687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def promptFormattingFunction(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = alpacaPrompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass","metadata":{"execution":{"iopub.status.busy":"2024-07-02T12:47:08.939140Z","iopub.status.idle":"2024-07-02T12:47:08.940036Z","shell.execute_reply.started":"2024-07-02T12:47:08.939712Z","shell.execute_reply":"2024-07-02T12:47:08.939740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"code-search-net/code_search_net\", split=\"train[:1000]\")\ndataset = dataset.map(promptFormattingFunction, batched=True,)\n\n# Lowering test dataset size to train faster\ndatasetDictionary = dataset.train_test_split(test_size=0.004)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T12:47:08.941574Z","iopub.status.idle":"2024-07-02T12:47:08.942202Z","shell.execute_reply.started":"2024-07-02T12:47:08.941867Z","shell.execute_reply":"2024-07-02T12:47:08.941891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments(\n    per_device_train_batch_size = 2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps = 4,\n    evaluation_strategy=\"steps\",\n    warmup_ratio = 0.1,\n    num_train_epochs = 1,\n    learning_rate = 2e-5,\n    fp16 = not torch.cuda.is_bf16_supported(),\n    bf16 = torch.cuda.is_bf16_supported(),\n    optim = \"adamw_8bit\",\n    weight_decay = 0.1,\n    lr_scheduler_type = \"linear\",\n    seed = 3407,\n    output_dir = \"outputs\",\n    logging_steps = 1,\n    logging_strategy = 'steps',\n    save_total_limit = 2,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:40:14.079634Z","iopub.status.idle":"2024-07-02T09:40:14.079992Z","shell.execute_reply.started":"2024-07-02T09:40:14.079802Z","shell.execute_reply":"2024-07-02T09:40:14.079816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset_dict[\"train\"],\n    eval_dataset = dataset_dict[\"test\"],\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = True, # Packs short sequences together to save time!\n    args = args,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:40:14.080794Z","iopub.status.idle":"2024-07-02T09:40:14.081129Z","shell.execute_reply.started":"2024-07-02T09:40:14.080970Z","shell.execute_reply":"2024-07-02T09:40:14.080984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:40:14.082591Z","iopub.status.idle":"2024-07-02T09:40:14.082912Z","shell.execute_reply.started":"2024-07-02T09:40:14.082756Z","shell.execute_reply":"2024-07-02T09:40:14.082769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Continue the fibonnaci sequence.\", # instruction\n        \"1, 1, 2, 3, 5, 8\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)","metadata":{},"execution_count":null,"outputs":[]}]}