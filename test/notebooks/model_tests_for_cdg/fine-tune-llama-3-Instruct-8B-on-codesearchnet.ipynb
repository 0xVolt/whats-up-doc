{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!mamba install --quiet --force-reinstall aiohttp -y\n!pip install -qU \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n!pip install -q \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n# !pip install wandb evaluate accelerate\n\n# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0 pprint\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-26T09:06:57.612369Z","iopub.execute_input":"2024-07-26T09:06:57.612733Z","iopub.status.idle":"2024-07-26T09:11:54.393254Z","shell.execute_reply.started":"2024-07-26T09:06:57.612704Z","shell.execute_reply":"2024-07-26T09:11:54.391947Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/c6f2354e.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/86b0f08d.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/c9ddbd6b.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/b121c3e7.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/497deca9.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/09cdf8bf.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/47929eba.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/3e39a7aa.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/2ce54b42.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/4ea078d6.json\" was modified by another program\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\n\u001b[33mWARNING: huggingface-hub 0.23.2 does not provide the extra 'hf-transfer'\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting datasets==2.16.0\n  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\nCollecting fsspec==2023.10.0\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nCollecting gcsfs==2023.10.0\n  Downloading gcsfs-2023.10.0-py2.py3-none-any.whl.metadata (1.6 kB)\n\u001b[31mERROR: Could not find a version that satisfies the requirement pprint (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for pprint\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nimport pprint as pp\nfrom datasets import load_dataset\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-07-26T09:14:23.573350Z","iopub.execute_input":"2024-07-26T09:14:23.573724Z","iopub.status.idle":"2024-07-26T09:14:23.578565Z","shell.execute_reply.started":"2024-07-26T09:14:23.573696Z","shell.execute_reply":"2024-07-26T09:14:23.577596Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"max_seq_length = 2048\ndtype = None\nload_in_4bit = True\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T09:12:23.396454Z","iopub.execute_input":"2024-07-26T09:12:23.397064Z","iopub.status.idle":"2024-07-26T09:12:53.663340Z","shell.execute_reply.started":"2024-07-26T09:12:23.397028Z","shell.execute_reply":"2024-07-26T09:12:53.662101Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"==((====))==  Unsloth: Fast Llama patching release 2024.8\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.25.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18052030a7a248c0be0ddb289736a4e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/131 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"458f3a5c7618491f8a7bcb50f47c406b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73f333b101fd41bcaf653ca325306f32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0167e195488479fbe07057433c806ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04b0a18c43da4d38bc2be89d75365903"}},"metadata":{}}]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 32,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\", # Set to True if out of memory\n    random_state = 3407,\n    use_rslora = False,\n    loftq_config = None,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T09:12:53.664652Z","iopub.execute_input":"2024-07-26T09:12:53.665002Z","iopub.status.idle":"2024-07-26T09:12:59.270766Z","shell.execute_reply.started":"2024-07-26T09:12:53.664968Z","shell.execute_reply":"2024-07-26T09:12:59.269733Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}]},{"cell_type":"code","source":"alpacaFormatString = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN (<|eot_id|>)\n\n# Define the formatting function to initially drop all the unnecessary columns and rename what we need\ndef formatFunctionSample(sample):\n    language = sample['language']\n    instruction = f\"What does this {language} function do?\"\n    inputText = sample['func_code_string']\n    outputText = sample['func_documentation_string']\n\n    # Returning a dictionary of the necessary columns\n    return {\n        \"instruction\": instruction,\n        \"input\": inputText,\n        \"output\": outputText\n    }\n\n# Define the function to create the new 'text' column\ndef createAlpacaFormatString(sample):\n    instruction = sample['instruction']\n    inputText = sample['input']\n    outputText = sample['output']\n    \n    text = alpacaFormatString.format(instruction, inputText, outputText) + EOS_TOKEN\n    sample['text'] = text\n    \n    return sample","metadata":{"execution":{"iopub.status.busy":"2024-07-26T09:44:33.870760Z","iopub.execute_input":"2024-07-26T09:44:33.871835Z","iopub.status.idle":"2024-07-26T09:44:33.879602Z","shell.execute_reply.started":"2024-07-26T09:44:33.871793Z","shell.execute_reply":"2024-07-26T09:44:33.878562Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"claudios/code_search_net\", \"python\", split=\"train[:21]\")\n\n# Mapping the existing dataset to the new format keeping only the keys of the dictionary we returned\ndataset = dataset.map(formatFunctionSample, remove_columns=dataset.column_names)\n\n# Adding the text column to the new dataset\ndataset = dataset.map(createAlpacaFormatString)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T09:45:34.119131Z","iopub.execute_input":"2024-07-26T09:45:34.119525Z","iopub.status.idle":"2024-07-26T09:45:35.624457Z","shell.execute_reply.started":"2024-07-26T09:45:34.119495Z","shell.execute_reply":"2024-07-26T09:45:35.623633Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c3ec4d75983438c96abec1c490a0120"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53712fbe5e9b40058a72f895e1819438"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-26T09:46:43.361790Z","iopub.execute_input":"2024-07-26T09:46:43.362808Z","iopub.status.idle":"2024-07-26T09:46:43.368627Z","shell.execute_reply.started":"2024-07-26T09:46:43.362771Z","shell.execute_reply":"2024-07-26T09:46:43.367761Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'input', 'output', 'text'],\n    num_rows: 21\n})"},"metadata":{}}]},{"cell_type":"code","source":"pp.pp(dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-07-26T09:45:40.600058Z","iopub.execute_input":"2024-07-26T09:45:40.600659Z","iopub.status.idle":"2024-07-26T09:45:40.606167Z","shell.execute_reply.started":"2024-07-26T09:45:40.600625Z","shell.execute_reply":"2024-07-26T09:45:40.605239Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"{'instruction': 'What does this python function do?',\n 'input': 'def addidsuffix(self, idsuffix, recursive = True):\\n'\n          '        \"\"\"Appends a suffix to this element\\'s ID, and optionally '\n          'to all child IDs as well. There is sually no need to call this '\n          'directly, invoked implicitly by :meth:`copy`\"\"\"\\n'\n          '        if self.id: self.id += idsuffix\\n'\n          '        if recursive:\\n'\n          '            for e in self:\\n'\n          '                try:\\n'\n          '                    e.addidsuffix(idsuffix, recursive)\\n'\n          '                except Exception:\\n'\n          '                    pass',\n 'output': \"Appends a suffix to this element's ID, and optionally to all child \"\n           'IDs as well. There is sually no need to call this directly, '\n           'invoked implicitly by :meth:`copy`',\n 'text': 'Below is an instruction that describes a task, paired with an input '\n         'that provides further context. Write a response that appropriately '\n         'completes the request.\\n'\n         '\\n'\n         '### Instruction:\\n'\n         'What does this python function do?\\n'\n         '\\n'\n         '### Input:\\n'\n         'def addidsuffix(self, idsuffix, recursive = True):\\n'\n         '        \"\"\"Appends a suffix to this element\\'s ID, and optionally to '\n         'all child IDs as well. There is sually no need to call this '\n         'directly, invoked implicitly by :meth:`copy`\"\"\"\\n'\n         '        if self.id: self.id += idsuffix\\n'\n         '        if recursive:\\n'\n         '            for e in self:\\n'\n         '                try:\\n'\n         '                    e.addidsuffix(idsuffix, recursive)\\n'\n         '                except Exception:\\n'\n         '                    pass\\n'\n         '\\n'\n         '### Response:\\n'\n         \"Appends a suffix to this element's ID, and optionally to all child \"\n         'IDs as well. There is sually no need to call this directly, invoked '\n         'implicitly by :meth:`copy`<|eot_id|>'}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(dataset[0]['text'])","metadata":{"execution":{"iopub.status.busy":"2024-07-26T09:45:48.078043Z","iopub.execute_input":"2024-07-26T09:45:48.078406Z","iopub.status.idle":"2024-07-26T09:45:48.083595Z","shell.execute_reply.started":"2024-07-26T09:45:48.078378Z","shell.execute_reply":"2024-07-26T09:45:48.082719Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nWhat does this python function do?\n\n### Input:\ndef addidsuffix(self, idsuffix, recursive = True):\n        \"\"\"Appends a suffix to this element's ID, and optionally to all child IDs as well. There is sually no need to call this directly, invoked implicitly by :meth:`copy`\"\"\"\n        if self.id: self.id += idsuffix\n        if recursive:\n            for e in self:\n                try:\n                    e.addidsuffix(idsuffix, recursive)\n                except Exception:\n                    pass\n\n### Response:\nAppends a suffix to this element's ID, and optionally to all child IDs as well. There is sually no need to call this directly, invoked implicitly by :meth:`copy`<|eot_id|>\n","output_type":"stream"}]},{"cell_type":"code","source":"datasetDictionary = dataset.train_test_split(test_size=0.33) ","metadata":{"execution":{"iopub.status.busy":"2024-07-26T09:47:51.699301Z","iopub.execute_input":"2024-07-26T09:47:51.699677Z","iopub.status.idle":"2024-07-26T09:47:51.734359Z","shell.execute_reply.started":"2024-07-26T09:47:51.699649Z","shell.execute_reply":"2024-07-26T09:47:51.733402Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"datasetDictionary","metadata":{"execution":{"iopub.status.busy":"2024-07-26T09:48:02.882495Z","iopub.execute_input":"2024-07-26T09:48:02.882874Z","iopub.status.idle":"2024-07-26T09:48:02.889210Z","shell.execute_reply.started":"2024-07-26T09:48:02.882842Z","shell.execute_reply":"2024-07-26T09:48:02.888288Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'input', 'output', 'text'],\n        num_rows: 14\n    })\n    test: Dataset({\n        features: ['instruction', 'input', 'output', 'text'],\n        num_rows: 7\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"args = TrainingArguments(\n    per_device_train_batch_size = 2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps = 4,\n    evaluation_strategy=\"steps\",\n    warmup_ratio = 0.1,\n    num_train_epochs = 1,\n    learning_rate = 2e-5,\n    fp16 = not torch.cuda.is_bf16_supported(),\n    bf16 = torch.cuda.is_bf16_supported(),\n    optim = \"adamw_8bit\",\n    weight_decay = 0.1,\n    lr_scheduler_type = \"linear\",\n    seed = 3407,\n    output_dir = \"outputs\",\n    logging_steps = 1,\n    logging_strategy = 'steps',\n    save_total_limit = 2,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T09:48:40.659197Z","iopub.execute_input":"2024-07-26T09:48:40.659582Z","iopub.status.idle":"2024-07-26T09:48:40.700652Z","shell.execute_reply.started":"2024-07-26T09:48:40.659555Z","shell.execute_reply":"2024-07-26T09:48:40.699711Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = datasetDictionary[\"train\"],\n    eval_dataset = datasetDictionary[\"test\"],\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = True, # Packs short sequences together to save time!\n    args = args,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T09:48:45.387823Z","iopub.execute_input":"2024-07-26T09:48:45.388450Z","iopub.status.idle":"2024-07-26T09:48:47.307726Z","shell.execute_reply.started":"2024-07-26T09:48:45.388421Z","shell.execute_reply":"2024-07-26T09:48:47.306768Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e3b9f9500b64140b49e09f52ce9f0ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"335bb585f8824219bdbc118850761c38"}},"metadata":{}}]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-26T09:48:47.309288Z","iopub.execute_input":"2024-07-26T09:48:47.309562Z","iopub.status.idle":"2024-07-26T09:48:47.315877Z","shell.execute_reply.started":"2024-07-26T09:48:47.309538Z","shell.execute_reply":"2024-07-26T09:48:47.314975Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.741 GB.\n5.762 GB of memory reserved.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T09:48:52.830683Z","iopub.execute_input":"2024-07-26T09:48:52.831055Z","iopub.status.idle":"2024-07-26T09:49:16.063608Z","shell.execute_reply.started":"2024-07-26T09:48:52.831026Z","shell.execute_reply":"2024-07-26T09:49:16.061872Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 2 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 1\n \"-____-\"     Number of trainable parameters = 83,886,080\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAbort\u001b[0m                                     Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m<string>:126\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n","File \u001b[0;32m<string>:286\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_callback.py:460\u001b[0m, in \u001b[0;36mCallbackHandler.on_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[1;32m    459\u001b[0m     control\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_begin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_callback.py:507\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 507\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/integrations/integration_utils.py:878\u001b[0m, in \u001b[0;36mWandbCallback.on_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m     args\u001b[38;5;241m.\u001b[39mrun_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialized:\n\u001b[0;32m--> 878\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/integrations/integration_utils.py:812\u001b[0m, in \u001b[0;36mWandbCallback.setup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39mtermwarn(\n\u001b[1;32m    806\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    808\u001b[0m             repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    809\u001b[0m         )\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWANDB_PROJECT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuggingface\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# add config parameters (run may have been created manually)\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mupdate(combined_dict, allow_val_change\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1178\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, settings)\u001b[0m\n\u001b[1;32m   1174\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in wandb.init()\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;66;03m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;66;03m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[0;32m-> 1178\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/analytics/sentry.py:155\u001b[0m, in \u001b[0;36mSentry.reraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception(exc)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# but hopefully it's not too bad\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1163\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, settings)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     wi \u001b[38;5;241m=\u001b[39m _WandbInit()\n\u001b[0;32m-> 1163\u001b[0m     \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wi\u001b[38;5;241m.\u001b[39minit()\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:300\u001b[0m, in \u001b[0;36m_WandbInit.setup\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m     settings\u001b[38;5;241m.\u001b[39mupdate(init_settings, source\u001b[38;5;241m=\u001b[39mSource\u001b[38;5;241m.\u001b[39mINIT)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m settings\u001b[38;5;241m.\u001b[39m_offline \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m settings\u001b[38;5;241m.\u001b[39m_noop:\n\u001b[0;32m--> 300\u001b[0m     \u001b[43mwandb_login\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_login\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43manonymous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manonymous\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforce\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_warning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_silent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_entity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# apply updated global state after login was handled\u001b[39;00m\n\u001b[1;32m    309\u001b[0m wl \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39msetup()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_login.py:334\u001b[0m, in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logged_in\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key:\n\u001b[0;32m--> 334\u001b[0m     \u001b[43mwlogin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# make sure login credentials get to the backend\u001b[39;00m\n\u001b[1;32m    337\u001b[0m wlogin\u001b[38;5;241m.\u001b[39mpropogate_login()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_login.py:256\u001b[0m, in \u001b[0;36m_WandbLogin.prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprompt_api_key\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 256\u001b[0m     key, status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prompt_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m ApiKeyStatus\u001b[38;5;241m.\u001b[39mNOTTY:\n\u001b[1;32m    258\u001b[0m         directive \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    259\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb login [your_api_key]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39m_cli_only_mode\n\u001b[1;32m    261\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb.login(key=[your_api_key])\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m         )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_login.py:236\u001b[0m, in \u001b[0;36m_WandbLogin._prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[43mapikey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_api_key\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m            \u001b[49m\u001b[43mno_offline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m            \u001b[49m\u001b[43mno_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# invalid key provided, try again\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermerror(e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/apikey.py:151\u001b[0m, in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    145\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermlog(\n\u001b[1;32m    146\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogging into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. (Learn how to deploy a W&B server locally: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwburls\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb_server\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         )\n\u001b[1;32m    148\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mtermlog(\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can find your API key in your browser here: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapp_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/authorize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m     )\n\u001b[0;32m--> 151\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43minput_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_ask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    152\u001b[0m write_key(settings, key, api\u001b[38;5;241m=\u001b[39mapi)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m key  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/click/termui.py:164\u001b[0m, in \u001b[0;36mprompt\u001b[0;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/click/termui.py:147\u001b[0m, in \u001b[0;36mprompt.<locals>.prompt_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hide_input:\n\u001b[1;32m    146\u001b[0m     echo(\u001b[38;5;28;01mNone\u001b[39;00m, err\u001b[38;5;241m=\u001b[39merr)\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Abort() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mAbort\u001b[0m: "],"ename":"Abort","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-26T09:49:16.064340Z","iopub.status.idle":"2024-07-26T09:49:16.064679Z","shell.execute_reply.started":"2024-07-26T09:49:16.064522Z","shell.execute_reply":"2024-07-26T09:49:16.064535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Continue the fibonnaci sequence.\", # instruction\n        \"1, 1, 2, 3, 5, 8\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T09:49:16.066029Z","iopub.status.idle":"2024-07-26T09:49:16.066434Z","shell.execute_reply.started":"2024-07-26T09:49:16.066242Z","shell.execute_reply":"2024-07-26T09:49:16.066273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}