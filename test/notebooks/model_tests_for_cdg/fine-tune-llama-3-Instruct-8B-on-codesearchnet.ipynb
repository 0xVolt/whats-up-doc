{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!mamba install --force-reinstall aiohttp -y\n!pip install -U \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n# !pip install wandb evaluate accelerate\n\n# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0\n\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048\ndtype = None\nload_in_4bit = True","metadata":{"execution":{"iopub.status.busy":"2024-06-26T23:33:36.001553Z","iopub.execute_input":"2024-06-26T23:33:36.002924Z","iopub.status.idle":"2024-06-26T23:33:36.008359Z","shell.execute_reply.started":"2024-06-26T23:33:36.002873Z","shell.execute_reply":"2024-06-26T23:33:36.007152Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T23:33:36.469293Z","iopub.execute_input":"2024-06-26T23:33:36.469706Z","iopub.status.idle":"2024-06-26T23:33:43.349513Z","shell.execute_reply.started":"2024-06-26T23:33:36.469664Z","shell.execute_reply":"2024-06-26T23:33:43.348680Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"==((====))==  Unsloth: Fast Llama patching release 2024.6\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    use_gradient_checkpointing = \"unsloth\", # 4x longer contexts auto supported!\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T23:33:43.351170Z","iopub.execute_input":"2024-06-26T23:33:43.351482Z","iopub.status.idle":"2024-06-26T23:33:48.701180Z","shell.execute_reply.started":"2024-06-26T23:33:43.351455Z","shell.execute_reply":"2024-06-26T23:33:48.700254Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True,)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T23:33:48.702312Z","iopub.execute_input":"2024-06-26T23:33:48.702613Z","iopub.status.idle":"2024-06-26T23:33:51.763752Z","shell.execute_reply.started":"2024-06-26T23:33:48.702586Z","shell.execute_reply":"2024-06-26T23:33:51.762803Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/11.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cf20bfbdb6f48fc8b0679c94b433e56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/44.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64882c7c63d044a6abd0187515694954"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e96606db30de4656921ac2b9f960868f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/51760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cba938004909432abdc084ac43143e88"}},"metadata":{}}]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        report_to = 'wandb',\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 60,\n        learning_rate = 2e-4,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\",\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T23:34:08.134182Z","iopub.execute_input":"2024-06-26T23:34:08.134564Z","iopub.status.idle":"2024-06-26T23:34:08.143381Z","shell.execute_reply.started":"2024-06-26T23:34:08.134535Z","shell.execute_reply":"2024-06-26T23:34:08.142069Z"},"trusted":true},"execution_count":13,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[13], line 27\u001b[0;36m\u001b[0m\n\u001b[0;31m    report_to = \"none\",\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword argument repeated: report_to\n"],"ename":"SyntaxError","evalue":"keyword argument repeated: report_to (145528703.py, line 27)","output_type":"error"}]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{},"execution_count":null,"outputs":[]}]}