{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/volt3000/fine-tune-llama-3-instruct-8b-on-codesearchnet-alp?scriptVersionId=190325582\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Fine-tune Llama-3-8B-Instruct with Unsloth on CodeSearchNet\n\n> Note: This notebooks runs best when it's accelerated with Nvidia T4(s) or GPU(s) of similar architecture ","metadata":{}},{"cell_type":"markdown","source":"## Download, Install and Import Dependencies","metadata":{}},{"cell_type":"code","source":"%%time\n!mamba install --quiet --force-reinstall aiohttp -y\n!pip install -qU \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n!pip install -q \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n# !pip install wandb evaluate accelerate\n\n# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-29T16:14:48.217154Z","iopub.execute_input":"2024-07-29T16:14:48.217511Z","iopub.status.idle":"2024-07-29T16:19:26.460613Z","shell.execute_reply.started":"2024-07-29T16:14:48.217482Z","shell.execute_reply":"2024-07-29T16:19:26.459476Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/c6f2354e.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/86b0f08d.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/c9ddbd6b.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/b121c3e7.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/497deca9.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/09cdf8bf.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/47929eba.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/3e39a7aa.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/2ce54b42.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/4ea078d6.json\" was modified by another program\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting datasets==2.16.0\n  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\nCollecting fsspec==2023.10.0\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nCollecting gcsfs==2023.10.0\n  Downloading gcsfs-2023.10.0-py2.py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (1.26.4)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (0.6)\nCollecting dill<0.3.8,>=0.3.0 (from datasets==2.16.0)\n  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (2.2.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (6.0.1)\nRequirement already satisfied: decorator>4.1.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs==2023.10.0) (5.1.1)\nRequirement already satisfied: google-auth>=1.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs==2023.10.0) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.10/site-packages (from gcsfs==2023.10.0) (1.2.0)\nRequirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.10/site-packages (from gcsfs==2023.10.0) (1.44.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (4.0.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs==2023.10.0) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs==2023.10.0) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs==2023.10.0) (4.9)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.16.0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.16.0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0) (2024.2.2)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib->gcsfs==2023.10.0) (1.3.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs==2023.10.0) (1.16.0)\nRequirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs==2023.10.0) (2.11.1)\nRequirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs==2023.10.0) (2.4.1)\nRequirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs==2023.10.0) (2.7.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs==2023.10.0) (3.20.3)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.16.0)\n  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0) (2023.4)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage->gcsfs==2023.10.0) (1.62.0)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage->gcsfs==2023.10.0) (1.5.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs==2023.10.0) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==2023.10.0) (3.2.2)\nDownloading datasets-2.16.0-py3-none-any.whl (507 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gcsfs-2023.10.0-py2.py3-none-any.whl (33 kB)\nDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, dill, multiprocess, datasets, gcsfs\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.3.1\n    Uninstalling fsspec-2024.3.1:\n      Successfully uninstalled fsspec-2024.3.1\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.8\n    Uninstalling dill-0.3.8:\n      Successfully uninstalled dill-0.3.8\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.16\n    Uninstalling multiprocess-0.70.16:\n      Successfully uninstalled multiprocess-0.70.16\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.19.2\n    Uninstalling datasets-2.19.2:\n      Successfully uninstalled datasets-2.19.2\n  Attempting uninstall: gcsfs\n    Found existing installation: gcsfs 2024.3.1\n    Uninstalling gcsfs-2024.3.1:\n      Successfully uninstalled gcsfs-2024.3.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\ns3fs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.16.0 dill-0.3.7 fsspec-2023.10.0 gcsfs-2023.10.0 multiprocess-0.70.15\nCPU times: user 3.34 s, sys: 718 ms, total: 4.06 s\nWall time: 4min 38s\n","output_type":"stream"}]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nimport pprint as pp\nfrom datasets import load_dataset\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:19:26.46267Z","iopub.execute_input":"2024-07-29T16:19:26.462996Z","iopub.status.idle":"2024-07-29T16:19:44.750352Z","shell.execute_reply.started":"2024-07-29T16:19:26.462958Z","shell.execute_reply":"2024-07-29T16:19:44.749561Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2024-07-29 16:19:34.801465: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-29 16:19:34.801559: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-29 16:19:34.928544: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Setup Model and Tokenizer from Unsloth","metadata":{}},{"cell_type":"code","source":"max_seq_length = 4096\ndtype = None\nload_in_4bit = True\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:19:44.751506Z","iopub.execute_input":"2024-07-29T16:19:44.752108Z","iopub.status.idle":"2024-07-29T16:20:13.571533Z","shell.execute_reply.started":"2024-07-29T16:19:44.75208Z","shell.execute_reply":"2024-07-29T16:20:13.570591Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.25.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c85805b0c13547ccadfbb36b8f218942"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/131 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6244edac7f646fe91a109a753749d9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b01b6360c844999aab88a127c54cf10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1609848696c84253bb859f35bdc6dea5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdeaa0385a714b48bcd291639ab32e48"}},"metadata":{}}]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 32,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = True, # Set to True if out of memory (default is \"unsloth\")\n    random_state = 8402,\n    use_rslora = False,\n    loftq_config = None,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:20:13.573917Z","iopub.execute_input":"2024-07-29T16:20:13.574245Z","iopub.status.idle":"2024-07-29T16:20:16.93652Z","shell.execute_reply.started":"2024-07-29T16:20:13.574218Z","shell.execute_reply":"2024-07-29T16:20:16.935732Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Format CodeSearchNet to Alpaca-styled Format","metadata":{}},{"cell_type":"code","source":"alpacaFormatString = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN (<|eot_id|>)\n\n# Define the formatting function to initially drop all the unnecessary columns and rename what we need\ndef formatFunctionSample(sample):\n    language = sample['language']\n    instruction = f\"What does this {language} function do?\"\n    inputText = sample['func_code_string']\n    outputText = sample['func_documentation_string']\n\n    # Returning a dictionary of the necessary columns\n    return {\n        \"instruction\": instruction,\n        \"input\": inputText,\n        \"output\": outputText\n    }\n\n# Define the function to create the new 'text' column\ndef createAlpacaFormatString(sample):\n    instruction = sample['instruction']\n    inputText = sample['input']\n    outputText = sample['output']\n    \n    text = alpacaFormatString.format(instruction, inputText, outputText) + EOS_TOKEN\n    sample['text'] = text\n    \n    return sample","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:20:16.937654Z","iopub.execute_input":"2024-07-29T16:20:16.937948Z","iopub.status.idle":"2024-07-29T16:20:17.919763Z","shell.execute_reply.started":"2024-07-29T16:20:16.937923Z","shell.execute_reply":"2024-07-29T16:20:17.918855Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"claudios/code_search_net\", \"python\", split=\"train[:42]\")\n\n# Mapping the existing dataset to the new format keeping only the keys of the dictionary we returned\ndataset = dataset.map(formatFunctionSample, remove_columns=dataset.column_names)\n\n# Adding the text column to the new dataset\ndataset = dataset.map(createAlpacaFormatString)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:21:57.825659Z","iopub.execute_input":"2024-07-29T16:21:57.826001Z","iopub.status.idle":"2024-07-29T16:21:59.279117Z","shell.execute_reply.started":"2024-07-29T16:21:57.825976Z","shell.execute_reply":"2024-07-29T16:21:59.278208Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/42 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3ade10c2adc4904b7278e7c54abbed4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/42 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d34ed79aad34e368308bf4894d7d363"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:21:59.280581Z","iopub.execute_input":"2024-07-29T16:21:59.280857Z","iopub.status.idle":"2024-07-29T16:21:59.287084Z","shell.execute_reply.started":"2024-07-29T16:21:59.280833Z","shell.execute_reply":"2024-07-29T16:21:59.286154Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'input', 'output', 'text'],\n    num_rows: 42\n})"},"metadata":{}}]},{"cell_type":"code","source":"pp.pp(dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:21:59.288271Z","iopub.execute_input":"2024-07-29T16:21:59.288596Z","iopub.status.idle":"2024-07-29T16:21:59.29687Z","shell.execute_reply.started":"2024-07-29T16:21:59.288565Z","shell.execute_reply":"2024-07-29T16:21:59.29588Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"{'instruction': 'What does this python function do?',\n 'input': 'def addidsuffix(self, idsuffix, recursive = True):\\n'\n          '        \"\"\"Appends a suffix to this element\\'s ID, and optionally '\n          'to all child IDs as well. There is sually no need to call this '\n          'directly, invoked implicitly by :meth:`copy`\"\"\"\\n'\n          '        if self.id: self.id += idsuffix\\n'\n          '        if recursive:\\n'\n          '            for e in self:\\n'\n          '                try:\\n'\n          '                    e.addidsuffix(idsuffix, recursive)\\n'\n          '                except Exception:\\n'\n          '                    pass',\n 'output': \"Appends a suffix to this element's ID, and optionally to all child \"\n           'IDs as well. There is sually no need to call this directly, '\n           'invoked implicitly by :meth:`copy`',\n 'text': 'Below is an instruction that describes a task, paired with an input '\n         'that provides further context. Write a response that appropriately '\n         'completes the request.\\n'\n         '\\n'\n         '### Instruction:\\n'\n         'What does this python function do?\\n'\n         '\\n'\n         '### Input:\\n'\n         'def addidsuffix(self, idsuffix, recursive = True):\\n'\n         '        \"\"\"Appends a suffix to this element\\'s ID, and optionally to '\n         'all child IDs as well. There is sually no need to call this '\n         'directly, invoked implicitly by :meth:`copy`\"\"\"\\n'\n         '        if self.id: self.id += idsuffix\\n'\n         '        if recursive:\\n'\n         '            for e in self:\\n'\n         '                try:\\n'\n         '                    e.addidsuffix(idsuffix, recursive)\\n'\n         '                except Exception:\\n'\n         '                    pass\\n'\n         '\\n'\n         '### Response:\\n'\n         \"Appends a suffix to this element's ID, and optionally to all child \"\n         'IDs as well. There is sually no need to call this directly, invoked '\n         'implicitly by :meth:`copy`<|eot_id|>'}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(dataset[0]['text'])","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:21:59.298691Z","iopub.execute_input":"2024-07-29T16:21:59.299059Z","iopub.status.idle":"2024-07-29T16:21:59.304946Z","shell.execute_reply.started":"2024-07-29T16:21:59.299036Z","shell.execute_reply":"2024-07-29T16:21:59.303968Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nWhat does this python function do?\n\n### Input:\ndef addidsuffix(self, idsuffix, recursive = True):\n        \"\"\"Appends a suffix to this element's ID, and optionally to all child IDs as well. There is sually no need to call this directly, invoked implicitly by :meth:`copy`\"\"\"\n        if self.id: self.id += idsuffix\n        if recursive:\n            for e in self:\n                try:\n                    e.addidsuffix(idsuffix, recursive)\n                except Exception:\n                    pass\n\n### Response:\nAppends a suffix to this element's ID, and optionally to all child IDs as well. There is sually no need to call this directly, invoked implicitly by :meth:`copy`<|eot_id|>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train-test Split","metadata":{}},{"cell_type":"code","source":"datasetDictionary = dataset.train_test_split(test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:22:01.086564Z","iopub.execute_input":"2024-07-29T16:22:01.087371Z","iopub.status.idle":"2024-07-29T16:22:01.099541Z","shell.execute_reply.started":"2024-07-29T16:22:01.087339Z","shell.execute_reply":"2024-07-29T16:22:01.098675Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"datasetDictionary","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:22:01.447764Z","iopub.execute_input":"2024-07-29T16:22:01.448113Z","iopub.status.idle":"2024-07-29T16:22:01.454052Z","shell.execute_reply.started":"2024-07-29T16:22:01.448084Z","shell.execute_reply":"2024-07-29T16:22:01.453026Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'input', 'output', 'text'],\n        num_rows: 29\n    })\n    test: Dataset({\n        features: ['instruction', 'input', 'output', 'text'],\n        num_rows: 13\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Initialize Trainer with Training Arguments","metadata":{}},{"cell_type":"code","source":"# Test trainer config with the evaluation loop to calculate validation loss\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = datasetDictionary[\"train\"],\n    eval_dataset = datasetDictionary[\"test\"],\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = True, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        per_device_eval_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        fp16_full_eval = True,\n        eval_accumulation_steps = 4,\n        evaluation_strategy = \"steps\",\n        eval_steps = 1,\n        warmup_steps = 5,\n        # max_steps = 60,\n        num_train_epochs = 1,\n        learning_rate = 2e-4,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 8402,\n        output_dir = \"outputs\",\n        report_to = \"none\",\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:22:07.454915Z","iopub.execute_input":"2024-07-29T16:22:07.455575Z","iopub.status.idle":"2024-07-29T16:22:07.963259Z","shell.execute_reply.started":"2024-07-29T16:22:07.455544Z","shell.execute_reply":"2024-07-29T16:22:07.962389Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84a4ab70910247308564990426ffee56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92aae9fb2be9490b98ad5ebb31789c15"}},"metadata":{}}]},{"cell_type":"code","source":"# trainer = SFTTrainer(\n#     model = model,\n#     tokenizer = tokenizer,\n#     train_dataset = datasetDictionary[\"train\"],\n#     eval_dataset = datasetDictionary[\"test\"],\n#     dataset_text_field = \"text\",\n#     max_seq_length = max_seq_length,\n#     dataset_num_proc = 2,\n#     packing = True, # Can make training 5x faster for short sequences.\n#     args = TrainingArguments(\n#         per_device_train_batch_size = 2,\n#         per_device_eval_batch_size = 2,\n#         gradient_accumulation_steps = 4,\n#         warmup_steps = 5,\n#         max_steps = 250,\n# #         num_train_epochs = 1,\n#         learning_rate = 2e-4,\n#         fp16 = not torch.cuda.is_bf16_supported(),\n#         bf16 = torch.cuda.is_bf16_supported(),\n#         logging_steps = 1,\n#         optim = \"adamw_8bit\",\n#         weight_decay = 0.01,\n#         lr_scheduler_type = \"linear\",\n#         seed = 8402,\n#         output_dir = \"outputs\",\n#         report_to = \"none\",\n#     ),\n# )","metadata":{"execution":{"iopub.status.busy":"2024-07-28T18:49:18.985836Z","iopub.execute_input":"2024-07-28T18:49:18.986166Z","iopub.status.idle":"2024-07-28T18:53:51.194467Z","shell.execute_reply.started":"2024-07-28T18:49:18.986135Z","shell.execute_reply":"2024-07-28T18:53:51.193525Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac1dab9791494b128a311ee6228415fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"261e39b62f974640a5c45b03bc381035"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:342: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:22:24.04732Z","iopub.execute_input":"2024-07-29T16:22:24.047704Z","iopub.status.idle":"2024-07-29T16:22:24.054407Z","shell.execute_reply.started":"2024-07-29T16:22:24.047673Z","shell.execute_reply":"2024-07-29T16:22:24.053374Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.741 GB.\n5.762 GB of memory reserved.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.eval_dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:22:25.569458Z","iopub.execute_input":"2024-07-29T16:22:25.570341Z","iopub.status.idle":"2024-07-29T16:22:25.576394Z","shell.execute_reply.started":"2024-07-29T16:22:25.570309Z","shell.execute_reply":"2024-07-29T16:22:25.575221Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'labels'],\n    num_rows: 1\n})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:22:28.983342Z","iopub.execute_input":"2024-07-29T16:22:28.98405Z","iopub.status.idle":"2024-07-29T16:22:28.989727Z","shell.execute_reply.started":"2024-07-29T16:22:28.984016Z","shell.execute_reply":"2024-07-29T16:22:28.988684Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'labels'],\n    num_rows: 3\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Fine-tune Training Loop","metadata":{}},{"cell_type":"code","source":"trainerStats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:22:37.061276Z","iopub.execute_input":"2024-07-29T16:22:37.061654Z","iopub.status.idle":"2024-07-29T16:23:22.810085Z","shell.execute_reply.started":"2024-07-29T16:22:37.061625Z","shell.execute_reply":"2024-07-29T16:23:22.809128Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 3 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 1\n \"-____-\"     Number of trainable parameters = 83,886,080\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 00:04, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.550900</td>\n      <td>0.999511</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory          / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(f\"{trainerStats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainerStats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:23:22.811634Z","iopub.execute_input":"2024-07-29T16:23:22.812206Z","iopub.status.idle":"2024-07-29T16:23:22.819519Z","shell.execute_reply.started":"2024-07-29T16:23:22.812178Z","shell.execute_reply":"2024-07-29T16:23:22.818555Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"43.2313 seconds used for training.\n0.72 minutes used for training.\nPeak reserved memory = 14.584 GB.\nPeak reserved memory for training = 8.822 GB.\nPeak reserved memory % of max memory = 98.935 %.\nPeak reserved memory for training % of max memory = 59.847 %.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Run Inference on Fine-tuned Model","metadata":{}},{"cell_type":"code","source":"from transformers import TextStreamer\n\ntextStreamer = TextStreamer(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:23:22.820736Z","iopub.execute_input":"2024-07-29T16:23:22.821346Z","iopub.status.idle":"2024-07-29T16:23:22.834377Z","shell.execute_reply.started":"2024-07-29T16:23:22.821311Z","shell.execute_reply":"2024-07-29T16:23:22.833469Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\ntestFunction = \"\"\"\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\"\"\"\n\ninputs = tokenizer(\n[\n    alpacaFormatString.format(\n        \"Explain this python function's functionality in 100 words.\", # instruction\n        testFunction, # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\n_ = model.generate(**inputs, streamer=textStreamer, max_new_tokens=1024)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}