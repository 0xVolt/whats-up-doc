{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/volt3000/fine-tune-llama-3-instruct-8b-on-codesearchnet-alp?scriptVersionId=190324767\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Fine-tune Llama-3-8B-Instruct with Unsloth on CodeSearchNet\n\n> Note: This notebooks runs best when it's accelerated with Nvidia T4(s) or GPU(s) of similar architecture ","metadata":{}},{"cell_type":"markdown","source":"## Download, Install and Import Dependencies","metadata":{}},{"cell_type":"code","source":"%%time\n!mamba install --quiet --force-reinstall aiohttp -y\n!pip install -qU \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n!pip install -q \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n# !pip install wandb evaluate accelerate\n\n# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-29T16:14:48.217154Z","iopub.execute_input":"2024-07-29T16:14:48.217511Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/c6f2354e.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/86b0f08d.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/c9ddbd6b.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/b121c3e7.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/497deca9.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/09cdf8bf.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/47929eba.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/3e39a7aa.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/2ce54b42.json\" was modified by another program\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/4ea078d6.json\" was modified by another program\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\n","output_type":"stream"}]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nimport pprint as pp\nfrom datasets import load_dataset\nimport torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup Model and Tokenizer from Unsloth","metadata":{}},{"cell_type":"code","source":"max_seq_length = 4096\ndtype = None\nload_in_4bit = True\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 32,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = True, # Set to True if out of memory (default is \"unsloth\")\n    random_state = 8402,\n    use_rslora = False,\n    loftq_config = None,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Format CodeSearchNet to Alpaca-styled Format","metadata":{}},{"cell_type":"code","source":"alpacaFormatString = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN (<|eot_id|>)\n\n# Define the formatting function to initially drop all the unnecessary columns and rename what we need\ndef formatFunctionSample(sample):\n    language = sample['language']\n    instruction = f\"What does this {language} function do?\"\n    inputText = sample['func_code_string']\n    outputText = sample['func_documentation_string']\n\n    # Returning a dictionary of the necessary columns\n    return {\n        \"instruction\": instruction,\n        \"input\": inputText,\n        \"output\": outputText\n    }\n\n# Define the function to create the new 'text' column\ndef createAlpacaFormatString(sample):\n    instruction = sample['instruction']\n    inputText = sample['input']\n    outputText = sample['output']\n    \n    text = alpacaFormatString.format(instruction, inputText, outputText) + EOS_TOKEN\n    sample['text'] = text\n    \n    return sample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"claudios/code_search_net\", \"python\", split=\"train[:21]\")\n\n# Mapping the existing dataset to the new format keeping only the keys of the dictionary we returned\ndataset = dataset.map(formatFunctionSample, remove_columns=dataset.column_names)\n\n# Adding the text column to the new dataset\ndataset = dataset.map(createAlpacaFormatString)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pp.pp(dataset[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset[0]['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train-test Split","metadata":{}},{"cell_type":"code","source":"datasetDictionary = dataset.train_test_split(test_size=0.004)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasetDictionary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initialize Trainer with Training Arguments","metadata":{}},{"cell_type":"code","source":"# Test trainer config with the evaluation loop to calculate validation loss\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = datasetDictionary[\"train\"],\n    eval_dataset = datasetDictionary[\"test\"],\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = True, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        per_device_eval_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        fp16_full_eval = True,\n        eval_accumulation_steps = 4,\n        evaluation_strategy = \"steps\",\n        eval_steps = 1,\n        warmup_steps = 5,\n        # max_steps = 60,\n        num_train_epochs = 1,\n        learning_rate = 2e-4,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 8402,\n        output_dir = \"outputs\",\n        report_to = \"none\",\n    ),\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer = SFTTrainer(\n#     model = model,\n#     tokenizer = tokenizer,\n#     train_dataset = datasetDictionary[\"train\"],\n#     eval_dataset = datasetDictionary[\"test\"],\n#     dataset_text_field = \"text\",\n#     max_seq_length = max_seq_length,\n#     dataset_num_proc = 2,\n#     packing = True, # Can make training 5x faster for short sequences.\n#     args = TrainingArguments(\n#         per_device_train_batch_size = 2,\n#         per_device_eval_batch_size = 2,\n#         gradient_accumulation_steps = 4,\n#         warmup_steps = 5,\n#         max_steps = 250,\n# #         num_train_epochs = 1,\n#         learning_rate = 2e-4,\n#         fp16 = not torch.cuda.is_bf16_supported(),\n#         bf16 = torch.cuda.is_bf16_supported(),\n#         logging_steps = 1,\n#         optim = \"adamw_8bit\",\n#         weight_decay = 0.01,\n#         lr_scheduler_type = \"linear\",\n#         seed = 8402,\n#         output_dir = \"outputs\",\n#         report_to = \"none\",\n#     ),\n# )","metadata":{"execution":{"iopub.status.busy":"2024-07-28T18:49:18.985836Z","iopub.execute_input":"2024-07-28T18:49:18.986166Z","iopub.status.idle":"2024-07-28T18:53:51.194467Z","shell.execute_reply.started":"2024-07-28T18:49:18.986135Z","shell.execute_reply":"2024-07-28T18:53:51.193525Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac1dab9791494b128a311ee6228415fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"261e39b62f974640a5c45b03bc381035"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:342: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T18:53:51.195613Z","iopub.execute_input":"2024-07-28T18:53:51.195853Z","iopub.status.idle":"2024-07-28T18:53:51.202241Z","shell.execute_reply.started":"2024-07-28T18:53:51.195831Z","shell.execute_reply":"2024-07-28T18:53:51.201277Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.741 GB.\n5.762 GB of memory reserved.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.eval_dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-28T18:53:51.203446Z","iopub.execute_input":"2024-07-28T18:53:51.203775Z","iopub.status.idle":"2024-07-28T18:53:51.211898Z","shell.execute_reply.started":"2024-07-28T18:53:51.203746Z","shell.execute_reply":"2024-07-28T18:53:51.21103Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'labels'],\n    num_rows: 139\n})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-28T18:53:51.212942Z","iopub.execute_input":"2024-07-28T18:53:51.213205Z","iopub.status.idle":"2024-07-28T18:53:51.221399Z","shell.execute_reply.started":"2024-07-28T18:53:51.213179Z","shell.execute_reply":"2024-07-28T18:53:51.220485Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'labels'],\n    num_rows: 35034\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Fine-tune Training Loop","metadata":{}},{"cell_type":"code","source":"trainerStats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-28T19:01:50.871947Z","iopub.execute_input":"2024-07-28T19:01:50.87285Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 35,034 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 250\n \"-____-\"     Number of trainable parameters = 83,886,080\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='71' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 71/250 1:56:47 < 5:03:00, 0.01 it/s, Epoch 0.02/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.303900</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.270000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.285500</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.205800</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.111400</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.227500</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.195100</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.192400</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.166000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.142300</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.194900</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.080900</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.084000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.128200</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.064000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.090300</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.093800</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.022600</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.098200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.060100</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.106900</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.044400</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.022500</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>1.007300</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.029200</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>1.120300</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>1.034600</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>1.141800</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>1.015500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.044900</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>1.037900</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>1.070600</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>1.057500</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>1.055100</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.020400</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>1.021600</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>1.080500</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>1.112700</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.988100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.062300</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>1.006700</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>1.062500</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>1.043000</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>1.077600</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>1.076800</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>1.006300</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>1.036200</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>1.097100</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>1.046400</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.965500</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>1.068000</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>1.061900</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>1.048100</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>1.002600</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.994000</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>1.058000</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>1.016100</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>1.044000</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>1.073500</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.011900</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>1.040000</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.982200</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.996900</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>1.041200</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>1.008500</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.984700</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>1.082800</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>1.015500</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>1.027500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory          / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(f\"{trainerStats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainerStats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference on Fine-tuned Model","metadata":{}},{"cell_type":"code","source":"from transformers import TextStreamer\n\ntextStreamer = TextStreamer(tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\ntestFunction = \"\"\"\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\"\"\"\n\ninputs = tokenizer(\n[\n    alpacaFormatString.format(\n        \"Explain this python function's functionality in 100 words.\", # instruction\n        testFunction, # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\n_ = model.generate(**inputs, streamer=textStreamer, max_new_tokens=1024)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}