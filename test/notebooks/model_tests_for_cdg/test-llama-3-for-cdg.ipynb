{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Inference on Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. [Model Card on HuggingFace](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T08:28:56.690549Z",
     "iopub.status.busy": "2024-06-15T08:28:56.689891Z",
     "iopub.status.idle": "2024-06-15T08:29:09.617511Z",
     "shell.execute_reply": "2024-06-15T08:29:09.616352Z",
     "shell.execute_reply.started": "2024-06-15T08:28:56.690520Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-06-15T08:29:12.928430Z",
     "iopub.status.busy": "2024-06-15T08:29:12.928060Z",
     "iopub.status.idle": "2024-06-15T08:29:28.552297Z",
     "shell.execute_reply": "2024-06-15T08:29:28.551543Z",
     "shell.execute_reply.started": "2024-06-15T08:29:12.928396Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 08:29:18.101209: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-15 08:29:18.101340: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-15 08:29:18.207548: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from huggingface_hub import notebook_login as hfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T08:29:31.347383Z",
     "iopub.status.busy": "2024-06-15T08:29:31.346413Z",
     "iopub.status.idle": "2024-06-15T08:29:31.368223Z",
     "shell.execute_reply": "2024-06-15T08:29:31.367286Z",
     "shell.execute_reply.started": "2024-06-15T08:29:31.347351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9417aaf02ef4b918a7cf7adbe08aa18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hfl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T08:29:40.405330Z",
     "iopub.status.busy": "2024-06-15T08:29:40.404985Z",
     "iopub.status.idle": "2024-06-15T08:32:13.512758Z",
     "shell.execute_reply": "2024-06-15T08:32:13.512009Z",
     "shell.execute_reply.started": "2024-06-15T08:29:40.405302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0990f02f6f74cfcb3b703c3daec48bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb250d48cad54286b827123ca81efddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d3af7ca98e4d68b28f51544ba6c5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a302d9b3f48f40a8a8f2838991281649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d768c2e2113240ca9df620d86d21533a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2553d0b8e49e4845b49848300a6591e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1dea197054a32be264cabe48fb834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387dc4da0c614600bfc5fe90bdfc3ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad41564d380a4af8a3da39a9b707ccb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f82c07552fa42389c9758bfa133cc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bbfce851134dbda3da06bd351dd816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a366c04aa3412dbd5a707094be058b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T08:32:22.098751Z",
     "iopub.status.busy": "2024-06-15T08:32:22.097876Z",
     "iopub.status.idle": "2024-06-15T08:32:22.106899Z",
     "shell.execute_reply": "2024-06-15T08:32:22.105956Z",
     "shell.execute_reply.started": "2024-06-15T08:32:22.098715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 16060.53 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T11:29:54.304345Z",
     "iopub.status.busy": "2024-06-12T11:29:54.303581Z",
     "iopub.status.idle": "2024-06-12T11:29:54.308760Z",
     "shell.execute_reply": "2024-06-12T11:29:54.307860Z",
     "shell.execute_reply.started": "2024-06-12T11:29:54.304300Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Write a script in Python that implements a circular queue and creates one with user input. Give an example of how the script works and document the code with appropriate comments and docstrings wherever necessary. \n",
    "Generate a response only for the prompt that is given and nothing else. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T11:29:55.368916Z",
     "iopub.status.busy": "2024-06-12T11:29:55.368579Z",
     "iopub.status.idle": "2024-06-12T11:29:55.373504Z",
     "shell.execute_reply": "2024-06-12T11:29:55.372586Z",
     "shell.execute_reply.started": "2024-06-12T11:29:55.368889Z"
    }
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful and masterful programming assistant that generates markdown documentation for any input program.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference with `model.generate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T11:29:56.860524Z",
     "iopub.status.busy": "2024-06-12T11:29:56.860064Z",
     "iopub.status.idle": "2024-06-12T11:29:56.934008Z",
     "shell.execute_reply": "2024-06-12T11:29:56.933084Z",
     "shell.execute_reply.started": "2024-06-12T11:29:56.860493Z"
    }
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T11:29:57.740235Z",
     "iopub.status.busy": "2024-06-12T11:29:57.739635Z",
     "iopub.status.idle": "2024-06-12T11:29:57.745655Z",
     "shell.execute_reply": "2024-06-12T11:29:57.744672Z",
     "shell.execute_reply.started": "2024-06-12T11:29:57.740205Z"
    }
   },
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T11:30:02.662013Z",
     "iopub.status.busy": "2024-06-12T11:30:02.661645Z",
     "iopub.status.idle": "2024-06-12T11:30:21.706445Z",
     "shell.execute_reply": "2024-06-12T11:30:21.705486Z",
     "shell.execute_reply.started": "2024-06-12T11:30:02.661982Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.3 s, sys: 39.6 ms, total: 18.3 s\n",
      "Wall time: 19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-06-12T11:30:21.708442Z",
     "iopub.status.busy": "2024-06-12T11:30:21.708134Z",
     "iopub.status.idle": "2024-06-12T11:30:21.713889Z",
     "shell.execute_reply": "2024-06-12T11:30:21.712847Z",
     "shell.execute_reply.started": "2024-06-12T11:30:21.708409Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a Python script that implements a circular queue and creates one with user input:\n",
      "```\n",
      "class CircularQueue:\n",
      "    \"\"\"\n",
      "    A circular queue implementation in Python.\n",
      "    \"\"\"\n",
      "    def __init__(self, size):\n",
      "        \"\"\"\n",
      "        Initialize the circular queue with a given size.\n",
      "\n",
      "        Args:\n",
      "            size (int): The size of the queue.\n",
      "        \"\"\"\n",
      "        self.size = size\n",
      "        self.queue = [None] * size\n",
      "        self.front = 0\n",
      "        self.rear = 0\n",
      "\n",
      "    def is_full(self):\n",
      "        \"\"\"\n",
      "        Check if the queue is full.\n",
      "\n",
      "        Returns:\n",
      "            bool: True if the queue is full, False otherwise.\n",
      "        \"\"\"\n",
      "        return (self.rear + 1) % self.size == self.front\n",
      "\n",
      "    def is_empty(self):\n",
      "        \"\"\"\n",
      "        Check if the queue is empty.\n",
      "\n",
      "        Returns:\n",
      "            bool: True if the queue is empty, False otherwise.\n",
      "        \"\"\"\n",
      "        return self.front == self.rear\n",
      "\n",
      "    def enqueue(self, item):\n",
      "        \"\"\"\n",
      "        Enqueue an item into the queue.\n",
      "\n",
      "        Args:\n",
      "            item: The item to be enqueued.\n",
      "        \"\"\"\n",
      "        if self.is_full():\n",
      "            print(\"Queue is full. Cannot enqueue.\")\n",
      "            return\n",
      "        self.queue[self.rear]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Generate with Custom Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T08:33:02.099151Z",
     "iopub.status.busy": "2024-06-15T08:33:02.098478Z",
     "iopub.status.idle": "2024-06-15T08:33:02.104292Z",
     "shell.execute_reply": "2024-06-15T08:33:02.103296Z",
     "shell.execute_reply.started": "2024-06-15T08:33:02.099098Z"
    }
   },
   "outputs": [],
   "source": [
    "language = \"Python\"\n",
    "code = \"\"\"\n",
    "def test_euler_method(self):\n",
    "    def dydx(x, y):\n",
    "        return x + y\n",
    "\n",
    "    x0, y0 = 0, 1\n",
    "    x_end = 1\n",
    "    h = 0.01\n",
    "    x, y = differential_equations.euler_method(dydx, x0, y0, x_end, h)\n",
    "    expected = np.exp(x_end) - x_end - 1  # Analytical solution for the differential equation dy/dx = x + y\n",
    "    self.assertAlmostEqual(y[-1], expected, places=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T08:33:02.767005Z",
     "iopub.status.busy": "2024-06-15T08:33:02.766255Z",
     "iopub.status.idle": "2024-06-15T08:33:02.772495Z",
     "shell.execute_reply": "2024-06-15T08:33:02.771699Z",
     "shell.execute_reply.started": "2024-06-15T08:33:02.766973Z"
    }
   },
   "outputs": [],
   "source": [
    "testAbstractCode = \"\"\"\n",
    "def f(self):\n",
    "    def a(x, y):\n",
    "        return x + y\n",
    "\n",
    "    x0, y0 = 0, 1\n",
    "    x_end = 1\n",
    "    h = 0.01\n",
    "    x, y = differential_equations.euler_method(dydx, x0, y0, x_end, h)\n",
    "    expected = np.exp(x_end) - x_end - 1\n",
    "    self.assertAlmostEqual(y[-1], expected, places=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T08:33:03.369992Z",
     "iopub.status.busy": "2024-06-15T08:33:03.369410Z",
     "iopub.status.idle": "2024-06-15T08:33:03.374813Z",
     "shell.execute_reply": "2024-06-15T08:33:03.373934Z",
     "shell.execute_reply.started": "2024-06-15T08:33:03.369958Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Given a script file in {language}, generate its documentation for each function. For each function in the script, document its name, arguments, return values, and a brief explanation of its logic. Ensure that code within comments is not parsed and documented. Generate nothing else than what is asked.\n",
    "\n",
    "Strictly use the following format for each function:\n",
    "\n",
    "## Function Name: `function_name`\n",
    "\n",
    "### Arguments\n",
    "* `arg1` (type): Description of argument 1.\n",
    "* `arg2` (type): Description of argument 2.\n",
    "* ...\n",
    "\n",
    "### Return Values\n",
    "* `return_value1` (type): Description of return value 1.\n",
    "* `return_value2` (type): Description of return value 2.\n",
    "* ...\n",
    "\n",
    "### Explanation of Function Logic:\n",
    "1. Brief explanation of the function logic step by step.\n",
    "2. ...\n",
    "3. ...\n",
    "\n",
    "-----------------------------------------------------------------------------\n",
    "\n",
    "Here is the function I want you to generate documentation of:\n",
    "\n",
    "{testAbstractCode}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T08:33:04.239346Z",
     "iopub.status.busy": "2024-06-15T08:33:04.238999Z",
     "iopub.status.idle": "2024-06-15T08:33:04.245997Z",
     "shell.execute_reply": "2024-06-15T08:33:04.245022Z",
     "shell.execute_reply.started": "2024-06-15T08:33:04.239317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGiven a script file in Python, generate its documentation for each function. For each function in the script, document its name, arguments, return values, and a brief explanation of its logic. Ensure that code within comments is not parsed and documented. Generate nothing else than what is asked.\\n\\nStrictly use the following format for each function:\\n\\n## Function Name: `function_name`\\n\\n### Arguments\\n* `arg1` (type): Description of argument 1.\\n* `arg2` (type): Description of argument 2.\\n* ...\\n\\n### Return Values\\n* `return_value1` (type): Description of return value 1.\\n* `return_value2` (type): Description of return value 2.\\n* ...\\n\\n### Explanation of Function Logic:\\n1. Brief explanation of the function logic step by step.\\n2. ...\\n3. ...\\n\\n-----------------------------------------------------------------------------\\n\\nHere is the function I want you to generate documentation of:\\n\\n\\ndef f(self):\\n    def a(x, y):\\n        return x + y\\n\\n    x0, y0 = 0, 1\\n    x_end = 1\\n    h = 0.01\\n    x, y = differential_equations.euler_method(dydx, x0, y0, x_end, h)\\n    expected = np.exp(x_end) - x_end - 1\\n    self.assertAlmostEqual(y[-1], expected, places=2)\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T08:33:05.722107Z",
     "iopub.status.busy": "2024-06-15T08:33:05.721258Z",
     "iopub.status.idle": "2024-06-15T08:33:21.876346Z",
     "shell.execute_reply": "2024-06-15T08:33:21.875419Z",
     "shell.execute_reply.started": "2024-06-15T08:33:05.722073Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 s, sys: 76.1 ms, total: 15.7 s\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful and masterful programming assistant that generates markdown documentation for any input program.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=1024,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T08:33:24.107389Z",
     "iopub.status.busy": "2024-06-15T08:33:24.106505Z",
     "iopub.status.idle": "2024-06-15T08:33:24.113533Z",
     "shell.execute_reply": "2024-06-15T08:33:24.112475Z",
     "shell.execute_reply.started": "2024-06-15T08:33:24.107354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Function Name: `f`\n",
      "\n",
      "### Arguments\n",
      "* `self` (object): Description of the object instance.\n",
      "* No other arguments.\n",
      "\n",
      "### Return Values\n",
      "* No return values.\n",
      "\n",
      "### Explanation of Function Logic:\n",
      "1. The function defines a nested function `a` that takes two arguments `x` and `y` and returns their sum.\n",
      "2. It then initializes several variables: `x0`, `y0` to 0 and 1 respectively, `x_end` to 1, and `h` to 0.01.\n",
      "3. It uses the `euler_method` function from `differential_equations` module to solve a differential equation, passing `dydx`, `x0`, `y0`, `x_end`, and `h` as arguments.\n",
      "4. It calculates the expected value using `np.exp(x_end) - x_end - 1`.\n",
      "5. Finally, it asserts that the last element of the solution `y` is close to the expected value using `assertAlmostEqual`.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "- This model works beautifully for our use case\n",
    "- We need a way to objectively quantify it's performance however"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
