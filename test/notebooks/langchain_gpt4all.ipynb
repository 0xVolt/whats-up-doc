{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LangChain with GPT4All and Custom Prompts\n",
    "\n",
    "## References\n",
    "\n",
    "1. [LangChain docs on using the GPT4All library](https://python.langchain.com/docs/integrations/llms/gpt4all)\n",
    "\n",
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = r\"C:\\Users\\deshi\\Code\\gpt4all-models\\mistral-7b-instruct-v0.1.Q4_0.gguf\"\n",
    "# local_path = r\"C:\\Users\\deshi\\Code\\gpt4all-models\\orca-mini-3b-gguf2-q4_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(\n",
    "    model=local_path, \n",
    "    callbacks=callbacks, \n",
    "    verbose=True,\n",
    "    # max_tokens=1024,\n",
    "    # n_predict=1024,\n",
    "    # n_batch=4,\n",
    "    # use_mlock=True,\n",
    "    n_threads=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmChain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Write a program to calculate the nth fibonacci number given a user input integer n. Also, print out all the n fibonacci numbers leading upto n in a list. Format the output in markdown format so it is easy to export to a markdown document.\n",
    "\"\"\"\n",
    "\n",
    "llmChain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Documentation Inference with Standard Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFunction = \"\"\"\n",
    "n = int(input(\"Enter an integer: \"))\n",
    "result = 0\n",
    "previous_two = 1\n",
    "while result < n:\n",
    "    next_two = previous_two + previous_two\n",
    "    result += next_two\n",
    "    previous_two = next_two\n",
    "    print(\"The nth Fibonacci number is:\", result)\n",
    "\"\"\"\n",
    "\n",
    "question = f\"\"\"\n",
    "Here's my function in Python:\\\n",
    "\n",
    "{testFunction}\\\n",
    "\n",
    "Given the definition of a function in any programming language (particularly Python and C++), please generate it's stand-alone documentation. I want it complete with fields like function name, function arguments and return values as well as a detailed explanation of how the function logic works line-by-line. Make it concise and informative to put the documentation into a project documentation file.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmChain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFunction = \"\"\"\n",
    "def get_llama_response(prompt: str) -> None:\n",
    "    '''\n",
    "    Generate a response from the Llama model.\n",
    "\n",
    "    Parameters:\n",
    "        prompt (str): The user's input/question for the model.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the model's response.\n",
    "    '''\n",
    "    sequences = llama_pipeline(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=256,\n",
    "    )\n",
    "    print(\"Chatbot:\", sequences[0]['generated_text'])\n",
    "\"\"\"\n",
    "\n",
    "language = \"Python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Here's my function in {language}:\n",
    "\n",
    "{function}\n",
    "\n",
    "Given the definition of a function in any programming language (particularly Python and C++), please generate it's stand-alone documentation. I want it complete with fields like function name, function arguments and return values as well as a detailed explanation of how the function logic works line-by-line. Make it concise and informative to put the documentation into a project documentation file.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template = \"\"\"\n",
    "# Given the definition of a function in any programming language, please generate it's documentation. The documentation needs to include fields like function name, function arguments and return values as well as a detailed explanation of how the function logic works line-by-line.\n",
    "\n",
    "# This is a function in {language}:\n",
    "# {function}\n",
    "\n",
    "# Generate it's documentation in the following format.\n",
    "\n",
    "# # Name of the function:\n",
    "# The function definition and body here.\n",
    "# # Arguments:\n",
    "# List of arguments along with their types. Display them like this:\n",
    "# :param n: An integer representing the position of the desired Fibonacci number in the sequence.\n",
    "# # Return values:\n",
    "# List of all the values returned by the function with their types. Display them like this.\n",
    "# :return: The nth Fibonacci number as an integer.\n",
    "# # Explanation:\n",
    "# Line-by-line explanation of the function's logic and implementation.\n",
    "# # Use cases:\n",
    "# How the function may be used in an intended context. Provide an example with code.\n",
    "# \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"language\", \"function\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmChain = LLMChain(\n",
    "    prompt=prompt, \n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here's my function in Python:\n",
      "\n",
      "\n",
      "def get_llama_response(prompt: str) -> None:\n",
      "    '''\n",
      "    Generate a response from the Llama model.\n",
      "\n",
      "    Parameters:\n",
      "        prompt (str): The user's input/question for the model.\n",
      "\n",
      "    Returns:\n",
      "        None: Prints the model's response.\n",
      "    '''\n",
      "    sequences = llama_pipeline(\n",
      "        prompt,\n",
      "        do_sample=True,\n",
      "        top_k=10,\n",
      "        num_return_sequences=1,\n",
      "        eos_token_id=tokenizer.eos_token_id,\n",
      "        max_length=256,\n",
      "    )\n",
      "    print(\"Chatbot:\", sequences[0]['generated_text'])\n",
      "\n",
      "\n",
      "Given the definition of a function in any programming language (particularly Python and C++), please generate it's stand-alone documentation. I want it complete with fields like function name, function arguments and return values as well as a detailed explanation of how the function logic works line-by-line. Make it concise and informative to put the documentation into a project documentation file.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(language=language, function=testFunction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Function Name: get_llama_response\n",
      "\n",
      "Description: This function generates a response from the Llama model based on user input.\n",
      "\n",
      "Arguments:\n",
      "- prompt (str): The user's input/question for the model.\n",
      "\n",
      "Returns:\n",
      "None: Prints the model's response.\n",
      "\n",
      "Explanation:\n",
      "The get_llama_response function takes a single argument, prompt, which is of type string. It then uses the llama_pipeline function from the Hugging Face Transformers library to generate a response from the Llama model based on the user's input.\n",
      "\n",
      "The llama_pipeline function returns a list of dictionaries, where each dictionary represents a generated sequence of tokens by the model. The first element in this list is used as the response for the chatbot.\n",
      "\n",
      "Finally, the function prints the generated text from the Llama model using the print() function.\n",
      "\n",
      "Example Usage:\n",
      "```python\n",
      "get_llama_response(\"What's your favorite color?\")\n",
      "```"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFunction Name: get_llama_response\\r\\n\\r\\nDescription: This function generates a response from the Llama model based on user input.\\r\\n\\r\\nArguments:\\r\\n- prompt (str): The user\\'s input/question for the model.\\r\\n\\r\\nReturns:\\r\\nNone: Prints the model\\'s response.\\r\\n\\r\\nExplanation:\\r\\nThe get_llama_response function takes a single argument, prompt, which is of type string. It then uses the llama_pipeline function from the Hugging Face Transformers library to generate a response from the Llama model based on the user\\'s input.\\r\\n\\r\\nThe llama_pipeline function returns a list of dictionaries, where each dictionary represents a generated sequence of tokens by the model. The first element in this list is used as the response for the chatbot.\\r\\n\\r\\nFinally, the function prints the generated text from the Llama model using the print() function.\\r\\n\\r\\nExample Usage:\\r\\n```python\\r\\nget_llama_response(\"What\\'s your favorite color?\")\\r\\n```'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmChain.run({\"language\": language, \"function\": testFunction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
