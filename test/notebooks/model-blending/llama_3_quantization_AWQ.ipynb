{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install autoawq\n!pip install nvidia-ml-py3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-26T16:13:15.736933Z","iopub.execute_input":"2024-05-26T16:13:15.737364Z","iopub.status.idle":"2024-05-26T16:13:49.843907Z","shell.execute_reply.started":"2024-05-26T16:13:15.737320Z","shell.execute_reply":"2024-05-26T16:13:49.842857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\nimport torch\nfrom huggingface_hub import notebook_login as hfl","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:13:49.846216Z","iopub.execute_input":"2024-05-26T16:13:49.846546Z","iopub.status.idle":"2024-05-26T16:14:17.498311Z","shell.execute_reply.started":"2024-05-26T16:13:49.846516Z","shell.execute_reply":"2024-05-26T16:14:17.497270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hfl()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:17.499541Z","iopub.execute_input":"2024-05-26T16:14:17.500126Z","iopub.status.idle":"2024-05-26T16:14:17.522751Z","shell.execute_reply.started":"2024-05-26T16:14:17.500098Z","shell.execute_reply":"2024-05-26T16:14:17.521830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_id = r\"0xVolt/Meta-Llama-3-8B-Instruct-Hermes-2-Pro-SLERP\"\nquantModelPath = r\"Meta-Llama-3-8B-Instruct-Hermes-2-Pro-SLERP-AWQ-4-bit\"\nquantConfig = {\n    \"q_group_size\": 128,\n    \"w_bit\": 4\n}","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-26T16:14:36.680784Z","iopub.execute_input":"2024-05-26T16:14:36.681576Z","iopub.status.idle":"2024-05-26T16:14:36.686838Z","shell.execute_reply.started":"2024-05-26T16:14:36.681542Z","shell.execute_reply":"2024-05-26T16:14:36.685713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accidentally used `AutoModelForCausalLM` as a consequence of muscle memory. Should be `AutoAWQForCausalLM` :)","metadata":{}},{"cell_type":"code","source":"model = AutoAWQForCausalLM.from_pretrained(\n        model_id,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:36.688588Z","iopub.execute_input":"2024-05-26T16:14:36.688949Z","iopub.status.idle":"2024-05-26T16:16:12.974183Z","shell.execute_reply.started":"2024-05-26T16:14:36.688924Z","shell.execute_reply":"2024-05-26T16:16:12.972980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodel.quantize(tokenizer, quant_config=quantConfig)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:16:12.975754Z","iopub.execute_input":"2024-05-26T16:16:12.976203Z","iopub.status.idle":"2024-05-26T17:05:51.823805Z","shell.execute_reply.started":"2024-05-26T16:16:12.976164Z","shell.execute_reply":"2024-05-26T17:05:51.822813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodel.save_quantized(\"./\" + quantModelPath, safetensors=True)\ntokenizer.save_pretrained(\"./\" + quantModelPath, use_fast=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T17:05:51.825872Z","iopub.execute_input":"2024-05-26T17:05:51.826180Z","iopub.status.idle":"2024-05-26T17:06:05.586920Z","shell.execute_reply.started":"2024-05-26T17:05:51.826154Z","shell.execute_reply":"2024-05-26T17:06:05.585862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nuserSecrets = UserSecretsClient()\nHF_WRITE_TOKEN = userSecrets.get_secret(\"HF_WRITE_TOKEN\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T17:13:30.648590Z","iopub.execute_input":"2024-05-26T17:13:30.649186Z","iopub.status.idle":"2024-05-26T17:13:30.794116Z","shell.execute_reply.started":"2024-05-26T17:13:30.649123Z","shell.execute_reply":"2024-05-26T17:13:30.793131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\nusername = \"0xVolt\"\nmodelName = \"Meta-Llama-3-8B-Instruct-Hermes-2-Pro-SLERP-AWQ-4-bit\"\noutputDir = r\"/kaggle/working/Meta-Llama-3-8B-Instruct-Hermes-2-Pro-SLERP-AWQ-4-bit\"\n\n# Defined in the secrets tab in Kaggle Secrets\napi = HfApi(token=HF_WRITE_TOKEN)\n\napi.create_repo(\n    repo_id=f\"{username}/{modelName}\",\n    repo_type=\"model\"\n)\n\n# Push the whole merged model's folder to the hub\napi.upload_folder(\n    repo_id=f\"{username}/{modelName}\",\n    folder_path=outputDir,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T17:15:18.974028Z","iopub.execute_input":"2024-05-26T17:15:18.975041Z","iopub.status.idle":"2024-05-26T17:17:19.636672Z","shell.execute_reply.started":"2024-05-26T17:15:18.975006Z","shell.execute_reply":"2024-05-26T17:17:19.635601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}