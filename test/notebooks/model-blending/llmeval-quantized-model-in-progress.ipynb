{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install autoawq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login as hfl\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom datetime import datetime\nfrom pynvml import *\nimport awq\nimport torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hfl()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def printGPUState():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    \n    print(f\"GPU Memory Occupied: {info.used // 1024 ** 2} MB.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# model_id = \"0xVolt/Meta-Llama-3-8B-Instruct-Hermes-2-Pro-SLERP-AWQ-4-bit\"\n# model_id = \"0xVolt/Meta-Llama-3-8B-Instruct-Hermes-2-Pro-SLERP\"\nmodel_id = \"0xVolt/Code-Mistral-Llama-7B-TIES\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, \n    torch_dtype=torch.float16, \n    device_map={\"\": 0}\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"printGPUState()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generateOutputWithModelPipeline(model, tokenizer, prompt, temperature=0.7): \n    pipe = pipeline(\n        task=\"text-generation\", \n        model=model, \n        tokenizer=tokenizer\n    )\n    \n    start = datetime.now()\n    \n    output = pipe(\n        prompt,\n        do_sample=True,\n#         max_new_tokens=3000, \n        temperature=temperature, \n        top_k=50, \n        top_p=0.95,\n        num_return_sequences=1\n    )\n    \n    stop = datetime.now()\n    \n    totalTimeToPrompt = stop - start\n    print(f\"Execution Time : {totalTimeToPrompt}\")\n    \n    return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"\nWrite a function in C++ that appends an element to the end of a linked list. Explain your code with comments as necessary.\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\noutput = generateOutputWithModelPipeline(model, tokenizer, prompt)\nprint(output[\"generated_text\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}