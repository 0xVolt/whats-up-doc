{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","include_colab_link":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/0xVolt/whats-up-doc/blob/main/test/notebooks/model-blending/blend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"# Merging CodeLLMs to Create an Efficant Low-Memory Quantized Model for `whats-up-doc` using the TIES Method","metadata":{"id":"dM0FHFhyY2g2"}},{"cell_type":"code","source":"import os\nimport yaml\nfrom transformers import AutoModelWithLMHead, AutoTokenizer, pipeline","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:00:57.662092Z","iopub.execute_input":"2024-05-19T17:00:57.662743Z","iopub.status.idle":"2024-05-19T17:01:12.565938Z","shell.execute_reply.started":"2024-05-19T17:00:57.662712Z","shell.execute_reply":"2024-05-19T17:01:12.565035Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-19 17:01:03.428995: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-19 17:01:03.429117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-19 17:01:03.511715: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Download and Install `mergekit`","metadata":{}},{"cell_type":"code","source":"dirName = \"mergekit\"\ncwd = os.getcwd()\n\nconcatDirPath = os.path.join(cwd, dirName)\n\nif not os.path.exists(concatDirPath):\n    !git clone https://github.com/cg123/mergekit.git\n    !cd mergekit && pip install -q -e .","metadata":{"id":"NPNPie5Eo3EZ","outputId":"b262d7b3-9f30-4725-a9db-7711af5ae836","execution":{"iopub.status.busy":"2024-05-19T17:01:12.568009Z","iopub.execute_input":"2024-05-19T17:01:12.569111Z","iopub.status.idle":"2024-05-19T17:01:57.108151Z","shell.execute_reply.started":"2024-05-19T17:01:12.569078Z","shell.execute_reply":"2024-05-19T17:01:57.107105Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'mergekit'...\nremote: Enumerating objects: 2265, done.\u001b[K\nremote: Counting objects: 100% (1354/1354), done.\u001b[K\nremote: Compressing objects: 100% (520/520), done.\u001b[K\nremote: Total 2265 (delta 1081), reused 947 (delta 833), pack-reused 911\u001b[K\nReceiving objects: 100% (2265/2265), 640.50 KiB | 5.93 MiB/s, done.\nResolving deltas: 100% (1584/1584), done.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.9.3 requires keras-core, which is not installed.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.6 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Login to HF with a Write API Key","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:02:48.241663Z","iopub.execute_input":"2024-05-19T17:02:48.242746Z","iopub.status.idle":"2024-05-19T17:02:48.264189Z","shell.execute_reply.started":"2024-05-19T17:02:48.242711Z","shell.execute_reply":"2024-05-19T17:02:48.263175Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5c8130eb48b4d63a577b5527efd8f93"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Create the YAML Config File to Merge Models with SLERP","metadata":{"id":"rU1g6RzPKt5s"}},{"cell_type":"markdown","source":"### Write Config Script","metadata":{"id":"VHwE6E7cLLOx"}},{"cell_type":"code","source":"# Set the resultant model's name\nMODEL_NAME = 'whats-up-llamas-ties'\n\nMODEL_1 = \"codellama/CodeLlama-7b-Instruct-hf\"\nMODEL_2 = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\nOUTPUT_DIR = \"whats-up-llamas-ties\"","metadata":{"id":"4zrN0st2U1_O","execution":{"iopub.status.busy":"2024-05-19T17:02:59.781642Z","iopub.execute_input":"2024-05-19T17:02:59.782022Z","iopub.status.idle":"2024-05-19T17:02:59.786580Z","shell.execute_reply.started":"2024-05-19T17:02:59.781993Z","shell.execute_reply":"2024-05-19T17:02:59.785533Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#### TIES YAML Config Creation\n\nWhat I've found to work is the model with the least `intermediate_size` param of the models is taken to be the base. The only explanation I can think of is that it works when going from a larger vector to a smaller vector, but not the other way around.\n\nFor example,\nThe `meta-llama/Meta-Llama-3-8B-Instruct` model's config looks like:\n```json\n{\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.40.0.dev0\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n```\n\nCompared to `codellama/CodeLlama-7b-Instruct-hf`'s:\n```json\n{\n  \"_name_or_path\": \"codellama/CodeLlama-7b-Instruct-hf\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 16384,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.33.0.dev0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32016\n}\n```\n\nObserve that the models `intermediate_size` are,\n`CodeLlama`'s = 11008 &\n`Llama-3`'s = 14336\n\nHence, my decision to take `CodeLlama` as the base model to merge `Llama-3` with.\n\n**More testing required.**","metadata":{"id":"ltwEFfdUkqXG"}},{"cell_type":"code","source":"yamlConfigTIESLlamas = f\"\"\"\nmodels:\n  - model: codellama/CodeLlama-7b-Instruct-hf  # no parameters necessary for base model\n  - model: meta-llama/Meta-Llama-3-8B-Instruct\n    parameters:\n      density: 0.5\n      weight: 0.5\nmerge_method: ties\nbase_model: codellama/CodeLlama-7b-Instruct-hf\nparameters:\n  normalize: true\n  int8_mask: true\ndtype: float16\n\"\"\"","metadata":{"id":"tOafmTvbkhM8","execution":{"iopub.status.busy":"2024-05-19T17:06:53.601423Z","iopub.execute_input":"2024-05-19T17:06:53.601833Z","iopub.status.idle":"2024-05-19T17:06:53.607191Z","shell.execute_reply.started":"2024-05-19T17:06:53.601801Z","shell.execute_reply":"2024-05-19T17:06:53.606317Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Save Config Script","metadata":{"id":"fT4cHC-9LPLO"}},{"cell_type":"code","source":"# Save the YAML configuration to a file\nyamlFileName = \"config.yaml\"\nwith open(yamlFileName, \"w\") as f:\n    f.write(yamlConfigTIESLlamas)","metadata":{"id":"mA1SMKrgQH39","execution":{"iopub.status.busy":"2024-05-19T17:06:54.887010Z","iopub.execute_input":"2024-05-19T17:06:54.887965Z","iopub.status.idle":"2024-05-19T17:06:54.893405Z","shell.execute_reply.started":"2024-05-19T17:06:54.887923Z","shell.execute_reply":"2024-05-19T17:06:54.892328Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Merge Models","metadata":{"id":"ked5Gn7NLXuw"}},{"cell_type":"code","source":"cmd = f\"mergekit-yaml {yamlFileName} {OUTPUT_DIR} --allow-crimes --copy-tokenizer --out-shard-size 1B --low-cpu-memory --write-model-card --lazy-unpickle\"\n!{cmd}","metadata":{"id":"o8XlZ9UmSfdS","outputId":"c1b1f3f9-a2ce-40e3-8eae-c5166e347587","scrolled":true,"execution":{"iopub.status.busy":"2024-05-19T17:06:56.066470Z","iopub.execute_input":"2024-05-19T17:06:56.067317Z","iopub.status.idle":"2024-05-19T17:12:58.364943Z","shell.execute_reply.started":"2024-05-19T17:06:56.067277Z","shell.execute_reply":"2024-05-19T17:12:58.363896Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Warmup loader cache:   0%|                                | 0/2 [00:00<?, ?it/s]\nFetching 11 files: 100%|█████████████████████| 11/11 [00:00<00:00, 45590.26it/s]\u001b[A\nWarmup loader cache:  50%|████████████            | 1/2 [00:00<00:00,  8.29it/s]\nFetching 10 files: 100%|█████████████████████| 10/10 [00:00<00:00, 18009.03it/s]\u001b[A\nWarmup loader cache: 100%|████████████████████████| 2/2 [00:00<00:00,  8.26it/s]\nExecuting graph: 100%|██████████████████████| 1457/1457 [05:50<00:00,  4.16it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Get the Write Token from Kaggle Notebook Secrets","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nuserSecrets = UserSecretsClient()\nHF_WRITE_TOKEN = userSecrets.get_secret(\"HF_WRITE_TOKEN\")","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:02:04.696251Z","iopub.execute_input":"2024-05-19T17:02:04.696550Z","iopub.status.idle":"2024-05-19T17:02:04.828365Z","shell.execute_reply.started":"2024-05-19T17:02:04.696525Z","shell.execute_reply":"2024-05-19T17:02:04.827602Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Use the HF API to Write the Model to a Repository","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\nusername = \"0xVolt\"\n\n# Defined in the secrets tab in Kaggle Secrets\napi = HfApi(token=HF_WRITE_TOKEN)\n\napi.create_repo(\n    repo_id=f\"{username}/{MODEL_NAME}\",\n    repo_type=\"model\"\n)\n\n# Push the whole merged-model folder to the hub\napi.upload_folder(\n    repo_id=f\"{username}/{MODEL_NAME}\",\n    folder_path=OUTPUT_DIR,\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-19T17:15:58.736924Z","iopub.execute_input":"2024-05-19T17:15:58.737330Z","iopub.status.idle":"2024-05-19T17:18:01.719231Z","shell.execute_reply.started":"2024-05-19T17:15:58.737299Z","shell.execute_reply":"2024-05-19T17:18:01.718296Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"model-00001-of-00014.safetensors:   0%|          | 0.00/929M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45982339c96746a495a8e6cd7095f4ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00014.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca506ff46061477193ed9980ce92e821"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00014.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da65c1bd610641379bb2b86060c3b1de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00014.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"341ddd433c24409fb68f1e83a9668361"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 15 LFS files:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"360724a50f4f4869aaf7fdbbc051cb21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00014.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92c549bab39345a5985d5525b4ddbecb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00014.safetensors:   0%|          | 0.00/944M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11dc4a61055a4407932d45a11670b0b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00014.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7207194291094aadb7c13a154f17418a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00014.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fefc236f3c742a7973a8e00f65fa6e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00009-of-00014.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a8586cdd26f478eba38ad5525dd112c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00010-of-00014.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cfe78c69e9c4c95b35285f2d48d37c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00011-of-00014.safetensors:   0%|          | 0.00/944M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5b95cc4437b4f4ab705013fe86f2341"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00012-of-00014.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c294686dd924349b6e444eebd2c7d28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00013-of-00014.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b805c880c1bc44f1bc062dadb5ac7047"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00014-of-00014.safetensors:   0%|          | 0.00/877M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd13590c07f244aab0d7c1dfcd964d55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"974933649c9b4f03a6259a053f6cacc6"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/0xVolt/whats-up-llamas-ties/commit/ee93e04fb153124d790f7c77930983ce019efa8b', commit_message='Upload folder using huggingface_hub', commit_description='', oid='ee93e04fb153124d790f7c77930983ce019efa8b', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Resultant Model's Config File","metadata":{}},{"cell_type":"code","source":"from transformers import AutoConfig\n\nconfig = AutoConfig.from_pretrained(\"0xVolt/whats-up-llamas-ties\")\nprint(config)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:19:25.611574Z","iopub.execute_input":"2024-05-19T17:19:25.612483Z","iopub.status.idle":"2024-05-19T17:19:25.743139Z","shell.execute_reply.started":"2024-05-19T17:19:25.612449Z","shell.execute_reply":"2024-05-19T17:19:25.742306Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/695 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01e9f221b9eb48a38ece923d2458b6d4"}},"metadata":{}},{"name":"stdout","text":"LlamaConfig {\n  \"_name_or_path\": \"0xVolt/whats-up-llamas-ties\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 16384,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.39.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 32016\n}\n\n","output_type":"stream"}]}]}