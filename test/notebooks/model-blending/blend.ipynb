{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0xVolt/whats-up-doc/blob/main/test/notebooks/model-blending/blend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM0FHFhyY2g2"
      },
      "source": [
        "# Merging CodeLLMs to Create an Efficant Low-Memory Quantized Model for `whats-up-doc`\n",
        "\n",
        "## Download and Install `mergekit`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPNPie5Eo3EZ",
        "outputId": "b262d7b3-9f30-4725-a9db-7711af5ae836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mergekit'...\n",
            "remote: Enumerating objects: 2265, done.\u001b[K\n",
            "remote: Counting objects: 100% (1338/1338), done.\u001b[K\n",
            "remote: Compressing objects: 100% (517/517), done.\u001b[K\n",
            "remote: Total 2265 (delta 1065), reused 935 (delta 820), pack-reused 927\u001b[K\n",
            "Receiving objects: 100% (2265/2265), 640.43 KiB | 2.43 MiB/s, done.\n",
            "Resolving deltas: 100% (1583/1583), done.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for mergekit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cg123/mergekit.git\n",
        "!cd mergekit && pip install -q -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU1g6RzPKt5s"
      },
      "source": [
        "## Create the YAML Config File to Merge Models with SLERP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ezsCJA5DX9eQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface-cli"
      ],
      "metadata": {
        "id": "RMb9YxyUUbaF",
        "outputId": "b94ec400-b1c3-42cf-b392-b07516aaee43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface-cli\n",
            "  Downloading huggingface_cli-0.1-py3-none-any.whl (1.0 kB)\n",
            "Installing collected packages: huggingface-cli\n",
            "Successfully installed huggingface-cli-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "FMArOq3PUdxZ",
        "outputId": "c04be935-e9e2-46cb-9afa-152598f6191e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) \n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHwE6E7cLLOx"
      },
      "source": [
        "### Write Config Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4zrN0st2U1_O"
      },
      "outputs": [],
      "source": [
        "# Set the resultant model's name\n",
        "MODEL_NAME = 'whats-up-llamas'\n",
        "\n",
        "MODEL_1 = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
        "MODEL_2 = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "OUTPUT_DIR = \"merged_model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXSSc9McU7mn"
      },
      "source": [
        "#### SLERP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yamlConfigSLERPLlamas = f\"\"\"\n",
        "slices:\n",
        "  - sources:\n",
        "      - model: {MODEL_1}\n",
        "        layer_range: [0, 32]\n",
        "      - model: {MODEL_2}\n",
        "        layer_range: [0, 32]\n",
        "merge_method: slerp\n",
        "base_model: {MODEL_1}\n",
        "parameters:\n",
        "  t:\n",
        "    - filter: self_attn\n",
        "      value: [0, 0.5, 0.3, 0.7, 1]\n",
        "    - filter: mlp\n",
        "      value: [1, 0.5, 0.7, 0.3, 0]\n",
        "    - value: 0.5\n",
        "dtype: float32\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "pxShzshZPS7Y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlTfKB0CVD9m"
      },
      "source": [
        "#### Passthrough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wo_0qYbaVFfM"
      },
      "outputs": [],
      "source": [
        "yamlConfigPassthrough = \"\"\"\n",
        "slices:\n",
        "  - sources:\n",
        "    - model: OpenPipe/mistral-ft-optimized-1218\n",
        "      layer_range: [0, 32]\n",
        "  - sources:\n",
        "    - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
        "      layer_range: [24, 32]\n",
        "merge_method: passthrough\n",
        "dtype: bfloat16\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cnNGANPLhTa"
      },
      "source": [
        "*Note: If you were to do this locally, instead of putting in the models' card name under `model`, you would specify the path to the model you downloaded from huggingface.*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DARE-TIES"
      ],
      "metadata": {
        "id": "ltwEFfdUkqXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yamlConfigDARETIESLlamas = f\"\"\"\n",
        "models:\n",
        "    # No parameters necessary for base model\n",
        "  - model: {MODEL_1}\n",
        "  - model: {MODEL_2}\n",
        "    parameters:\n",
        "      density: 0.53\n",
        "      weight: 0.4\n",
        "merge_method: dare_ties\n",
        "base_model: {MODEL_1}\n",
        "parameters:\n",
        "  int8_mask: true\n",
        "dtype: bfloat16\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tOafmTvbkhM8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT4cHC-9LPLO"
      },
      "source": [
        "### Save Config Script"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the YAML configuration to a file\n",
        "yamlFileName = \"config.yaml\"\n",
        "with open(yamlFileName, \"w\") as f:\n",
        "    f.write(yamlConfigDARETIESLlamas)"
      ],
      "metadata": {
        "id": "mA1SMKrgQH39"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ked5Gn7NLXuw"
      },
      "source": [
        "## Merge Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cmd = f\"mergekit-yaml {yamlFileName} {OUTPUT_DIR} --allow-crimes --copy-tokenizer --out-shard-size 1B --low-cpu-memory --write-model-card --lazy-unpickle\"\n",
        "!{cmd}"
      ],
      "metadata": {
        "id": "o8XlZ9UmSfdS",
        "outputId": "c1b1f3f9-a2ce-40e3-8eae-c5166e347587",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warmup loader cache:   0% 0/2 [00:00<?, ?it/s]\n",
            "Fetching 10 files: 100% 10/10 [00:00<00:00, 10787.82it/s]\n",
            "Warmup loader cache:  50% 1/2 [00:00<00:00,  1.61it/s]\n",
            "Fetching 11 files: 100% 11/11 [00:00<00:00, 30076.50it/s]\n",
            "Warmup loader cache: 100% 2/2 [00:01<00:00,  1.61it/s]\n",
            "Executing graph:   0% 3/1457 [00:00<06:24,  3.78it/s]WARNING:root:Using submatrix of meta-llama/Meta-Llama-3-8B-Instruct:lm_head.weight\n",
            "Executing graph:   1% 8/1457 [00:10<29:15,  1.21s/it]WARNING:root:Using submatrix of meta-llama/Meta-Llama-3-8B-Instruct:model.embed_tokens.weight\n",
            "Executing graph:   1% 10/1457 [00:20<1:08:39,  2.85s/it]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.0.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:   2% 24/1457 [00:21<13:43,  1.74it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.0.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:   2% 28/1457 [00:22<10:57,  2.17it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.0.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:   3% 37/1457 [00:23<04:02,  5.86it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.0.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:   4% 53/1457 [00:26<05:34,  4.20it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.0.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:   4% 64/1457 [00:27<03:12,  7.22it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.1.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:   5% 68/1457 [00:37<19:20,  1.20it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.1.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:   5% 73/1457 [00:38<11:33,  1.99it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.1.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:   6% 83/1457 [00:39<03:20,  6.86it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.1.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:   7% 98/1457 [00:41<04:13,  5.36it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.1.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:   7% 109/1457 [00:42<02:53,  7.78it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.10.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:   8% 113/1457 [00:44<04:32,  4.93it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.10.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:   8% 118/1457 [00:45<05:10,  4.31it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.10.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:   9% 127/1457 [00:45<01:51, 11.96it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.10.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  10% 143/1457 [00:48<04:40,  4.69it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.10.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  11% 154/1457 [00:49<03:02,  7.15it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.11.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  11% 158/1457 [00:51<04:43,  4.58it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.11.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  11% 163/1457 [00:52<05:31,  3.90it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.11.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  12% 173/1457 [00:59<10:49,  1.98it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.11.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  13% 188/1457 [01:02<05:24,  3.91it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.11.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  14% 199/1457 [01:03<03:08,  6.67it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.12.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  14% 203/1457 [01:04<04:20,  4.81it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.12.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  14% 208/1457 [01:05<05:04,  4.10it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.12.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  15% 218/1457 [01:05<01:49, 11.30it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.12.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  16% 232/1457 [01:08<04:17,  4.76it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.12.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  17% 244/1457 [01:09<02:32,  7.93it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.13.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  17% 248/1457 [01:10<03:44,  5.37it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.13.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  17% 253/1457 [01:11<04:29,  4.48it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.13.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  18% 262/1457 [01:12<01:43, 11.58it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.13.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  19% 278/1457 [01:20<13:33,  1.45it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.13.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  20% 289/1457 [01:21<05:31,  3.53it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.14.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  20% 293/1457 [01:22<05:26,  3.57it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.14.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  20% 298/1457 [01:23<04:59,  3.87it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.14.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  21% 308/1457 [01:24<01:40, 11.43it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.14.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  22% 323/1457 [01:27<03:22,  5.60it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.14.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  23% 334/1457 [01:28<02:24,  7.77it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.15.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  23% 338/1457 [01:29<03:52,  4.82it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.15.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  24% 343/1457 [01:30<04:55,  3.77it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.15.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  24% 353/1457 [01:30<01:36, 11.44it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.15.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  25% 368/1457 [01:34<04:07,  4.39it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.15.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  26% 379/1457 [01:35<02:32,  7.09it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.16.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  26% 383/1457 [01:36<03:38,  4.91it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.16.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  27% 388/1457 [01:54<34:41,  1.95s/it]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.16.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  27% 398/1457 [01:54<07:43,  2.29it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.16.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  28% 413/1457 [01:57<03:49,  4.56it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.16.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  29% 424/1457 [01:58<02:22,  7.23it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.17.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  29% 428/1457 [01:59<03:27,  4.95it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.17.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  30% 433/1457 [02:00<03:56,  4.32it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.17.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  30% 442/1457 [02:00<01:32, 10.92it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.17.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  31% 458/1457 [02:03<02:52,  5.78it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.17.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  32% 469/1457 [02:04<02:07,  7.75it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.18.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  32% 473/1457 [02:05<03:26,  4.76it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.18.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  33% 478/1457 [02:07<03:58,  4.11it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.18.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  33% 487/1457 [02:07<01:25, 11.36it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.18.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  35% 503/1457 [02:15<03:56,  4.04it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.18.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  35% 514/1457 [02:16<02:19,  6.77it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.19.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  36% 518/1457 [02:17<03:24,  4.60it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.19.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  36% 523/1457 [02:18<03:51,  4.03it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.19.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  37% 532/1457 [02:18<01:17, 11.86it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.19.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  38% 548/1457 [02:22<03:40,  4.11it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.19.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  38% 559/1457 [02:23<02:11,  6.82it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.2.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  39% 563/1457 [02:24<03:12,  4.65it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.2.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  39% 568/1457 [02:25<03:34,  4.14it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.2.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  40% 577/1457 [02:25<01:17, 11.34it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.2.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  41% 592/1457 [02:29<03:18,  4.36it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.2.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  41% 604/1457 [02:30<01:59,  7.16it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.20.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  42% 608/1457 [02:47<24:12,  1.71s/it]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.20.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  42% 613/1457 [02:48<10:59,  1.28it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.20.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  43% 623/1457 [02:48<02:41,  5.17it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.20.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  44% 638/1457 [02:51<02:37,  5.21it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.20.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  45% 649/1457 [02:52<01:43,  7.84it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.21.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  45% 653/1457 [02:53<02:56,  4.54it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.21.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  45% 658/1457 [02:54<03:14,  4.10it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.21.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  46% 667/1457 [02:54<01:09, 11.37it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.21.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  47% 682/1457 [02:57<02:30,  5.15it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.21.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  48% 694/1457 [02:58<01:31,  8.33it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.22.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  48% 698/1457 [02:59<02:20,  5.42it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.22.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  48% 703/1457 [03:00<02:40,  4.69it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.22.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  49% 712/1457 [03:15<13:14,  1.07s/it]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.22.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  50% 728/1457 [03:18<04:10,  2.91it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.22.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  51% 739/1457 [03:19<02:09,  5.54it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.23.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  51% 743/1457 [03:20<02:41,  4.43it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.23.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  51% 748/1457 [03:21<02:51,  4.14it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.23.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  52% 757/1457 [03:22<01:04, 10.84it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.23.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  53% 773/1457 [03:24<02:04,  5.50it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.23.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  54% 784/1457 [03:25<01:28,  7.60it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.24.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  54% 788/1457 [03:27<02:17,  4.85it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.24.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  54% 793/1457 [03:28<02:42,  4.08it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.24.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  55% 803/1457 [03:28<00:55, 11.84it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.24.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  56% 817/1457 [03:45<20:20,  1.91s/it]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.24.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  57% 829/1457 [03:46<05:29,  1.91it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.25.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  57% 833/1457 [03:47<04:28,  2.32it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.25.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  58% 838/1457 [03:48<03:24,  3.02it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.25.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  58% 848/1457 [03:49<01:03,  9.67it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.25.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  59% 862/1457 [03:51<01:51,  5.34it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.25.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  60% 874/1457 [03:52<01:10,  8.32it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.26.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  60% 878/1457 [03:53<01:49,  5.27it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.26.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  61% 883/1457 [03:54<02:10,  4.39it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.26.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  61% 893/1457 [03:55<00:47, 11.75it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.26.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  62% 907/1457 [03:58<02:19,  3.94it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.26.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  63% 919/1457 [03:59<01:17,  6.99it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.27.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  63% 923/1457 [04:00<01:45,  5.05it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.27.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  64% 928/1457 [04:06<06:24,  1.38it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.27.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  64% 937/1457 [04:07<01:41,  5.10it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.27.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  65% 952/1457 [04:10<01:59,  4.22it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.27.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  66% 964/1457 [04:11<01:08,  7.21it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.28.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  66% 968/1457 [04:12<01:43,  4.70it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.28.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  67% 973/1457 [04:13<01:59,  4.06it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.28.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  67% 982/1457 [04:14<00:42, 11.23it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.28.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  68% 997/1457 [04:16<01:28,  5.20it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.28.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  69% 1009/1457 [04:17<00:56,  7.96it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.29.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  70% 1013/1457 [04:18<01:24,  5.28it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.29.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  70% 1018/1457 [04:19<01:37,  4.50it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.29.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  70% 1027/1457 [04:20<00:37, 11.54it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.29.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  72% 1043/1457 [04:41<03:05,  2.23it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.29.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  72% 1054/1457 [04:42<01:22,  4.89it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.3.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  73% 1058/1457 [04:43<01:36,  4.15it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.3.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  73% 1063/1457 [04:44<01:34,  4.16it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.3.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  74% 1073/1457 [04:44<00:32, 11.82it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.3.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  75% 1088/1457 [04:48<01:23,  4.41it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.3.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  75% 1099/1457 [04:49<00:51,  6.98it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.30.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  76% 1103/1457 [04:50<01:12,  4.91it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.30.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  76% 1108/1457 [04:51<01:19,  4.37it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.30.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  77% 1117/1457 [04:51<00:28, 11.80it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.30.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  78% 1133/1457 [04:54<00:56,  5.79it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.30.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  79% 1144/1457 [04:55<00:38,  8.13it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.31.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  79% 1148/1457 [05:01<02:46,  1.86it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.31.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  79% 1153/1457 [05:02<01:56,  2.62it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.31.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  80% 1162/1457 [05:02<00:36,  8.19it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.31.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  81% 1178/1457 [05:05<00:51,  5.45it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.31.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  82% 1189/1457 [05:06<00:35,  7.63it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.4.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  82% 1193/1457 [05:07<00:51,  5.16it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.4.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  82% 1198/1457 [05:08<00:59,  4.33it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.4.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  83% 1208/1457 [05:09<00:21, 11.73it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.4.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  84% 1223/1457 [05:12<00:48,  4.83it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.4.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  85% 1234/1457 [05:13<00:29,  7.47it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.5.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  85% 1238/1457 [05:14<00:45,  4.83it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.5.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  85% 1243/1457 [05:15<00:49,  4.37it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.5.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  86% 1252/1457 [05:35<05:13,  1.53s/it]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.5.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  87% 1268/1457 [05:38<01:11,  2.64it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.5.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  88% 1279/1457 [05:39<00:32,  5.40it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.6.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  88% 1283/1457 [05:40<00:40,  4.34it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.6.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  88% 1288/1457 [05:41<00:41,  4.07it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.6.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  89% 1298/1457 [05:42<00:14, 11.24it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.6.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  90% 1312/1457 [05:45<00:32,  4.47it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.6.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  91% 1324/1457 [05:46<00:18,  7.32it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.7.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  91% 1328/1457 [05:47<00:25,  4.97it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.7.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  91% 1333/1457 [05:48<00:30,  4.11it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.7.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  92% 1342/1457 [05:49<00:10, 10.80it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.7.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  93% 1358/1457 [05:57<01:08,  1.44it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.7.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  94% 1369/1457 [05:58<00:23,  3.72it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.8.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  94% 1373/1457 [05:59<00:23,  3.63it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.8.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  95% 1378/1457 [06:00<00:21,  3.74it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.8.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  95% 1388/1457 [06:00<00:06, 11.32it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.8.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  96% 1402/1457 [06:03<00:12,  4.44it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.8.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  97% 1414/1457 [06:04<00:06,  7.03it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.9.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  97% 1418/1457 [06:06<00:08,  4.71it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.9.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  98% 1423/1457 [06:07<00:07,  4.33it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.9.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  98% 1433/1457 [06:07<00:01, 12.04it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.9.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  99% 1448/1457 [06:10<00:01,  6.03it/s]WARNING:root:skipping meta-llama/Meta-Llama-3-8B-Instruct:model.layers.9.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph: 100% 1457/1457 [06:18<00:00,  3.85it/s]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}