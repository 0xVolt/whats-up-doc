{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0xVolt/whats-up-doc/blob/main/src/experimental-notebooks/fine-tune-distilbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA6OHGEUzKEH"
      },
      "source": [
        "# Fine-tuning the DistilBERT Model from HuggingFace\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z8-GdTjD2oED"
      },
      "outputs": [],
      "source": [
        "%pip install transformers --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usGBfifzzKEM"
      },
      "source": [
        "## Import data to fine-tune model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dCrR0FFPzKEO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r8p_OZ_wzKEQ"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('spam-data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXcVi2QCzKER",
        "outputId": "acff860b-be2a-4d56-ea2d-e7356ec33baa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(   label                                               text\n",
              " 0      0  Go until jurong point, crazy.. Available only ...\n",
              " 1      0                      Ok lar... Joking wif u oni...\n",
              " 2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              " 3      0  U dun say so early hor... U c already then say...\n",
              " 4      0  Nah I don't think he goes to usf, he lives aro...,\n",
              " (5572, 2))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.head(), dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znpR2NggzKEU"
      },
      "source": [
        "## Extract dependent and independent features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A7cmdmTdzKEV"
      },
      "outputs": [],
      "source": [
        "X = list(dataset['text'])\n",
        "y = list(dataset['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k14UFUD0zKEV"
      },
      "source": [
        "## Train-test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sBeYrbtuzKEW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzNcmq9OzKEW"
      },
      "source": [
        "## Use a HuggingFace Model\n",
        "\n",
        "Generally, the steps involved in using a model from HuggingFace involves,\n",
        "1. Calling the pre-trained model\n",
        "2. Calling the model's tokenizer - since each model has it's own tokenizer\n",
        "3. Use the tokenizer to encode the train and test datasets\n",
        "   1. `truncation` - remove whitespace from each data point\n",
        "   2. `padding` - conform all data points to the same length\n",
        "4. Create dataset objects in whichever framework you use, i.e., either Tensorflow tensors or the PyTorch equivalent\n",
        "5. Create training arguments\n",
        "6. Create a trainer with the training arguments and training data\n",
        "7. Train the model with the trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7cATB22jzKEX"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GfqBXCaFzKEY"
      },
      "outputs": [],
      "source": [
        "trainEncoded = tokenizer(X_train, truncation=True, padding=True)\n",
        "testEncoded = tokenizer(X_test, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpAJ_m3kzKEZ"
      },
      "source": [
        "## Create Dataset Objects with Tensorflow\n",
        "\n",
        "In tensorflow, the dataset objects are tensors. We do this so data flows through our pipeline in the expected format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Uq17Rqn7zKEa"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "trainDataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(trainEncoded),\n",
        "    y_train\n",
        "))\n",
        "\n",
        "testDataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(testEncoded),\n",
        "    y_test\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "brRe9x5ezKEb"
      },
      "outputs": [],
      "source": [
        "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
        "\n",
        "trainingArguments = TFTrainingArguments(\n",
        "    output_dir = './results',\n",
        "    num_train_epochs = 2,\n",
        "    evaluation_strategy = 'steps',\n",
        "    eval_steps = 500,\n",
        "    per_device_train_batch_size = 4,\n",
        "    per_device_eval_batch_size = 8,\n",
        "    warmup_steps = 100,\n",
        "    weight_decay = 0.01,\n",
        "    logging_dir = './logs',\n",
        "    logging_steps = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEupt2fqzsS_",
        "outputId": "20137e78-be0a-4fc7-9bdf-fbd86cdc22a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "with trainingArguments.strategy.scope():\n",
        "    model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3hOiNhR0G7V",
        "outputId": "33ddcbf5-53bf-4f53-c937-1c7052366230"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/trainer_tf.py:118: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "trainer = TFTrainer(\n",
        "    model = model,\n",
        "    args = trainingArguments,\n",
        "    train_dataset = trainDataset,\n",
        "    eval_dataset = testDataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zYDyQxYI8S5x",
        "outputId": "97c844c5-3dc2-4652-a99d-205bb23fa2ff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "P7sHru_30Y7v"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
