{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0xVolt/whats-up-doc/blob/main/src/experimental-notebooks/fine-tune-distilbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA6OHGEUzKEH"
      },
      "source": [
        "# Fine-tuning the DistilBERT Model from HuggingFace\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usGBfifzzKEM"
      },
      "source": [
        "## Import data to fine-tune model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "dCrR0FFPzKEO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "r8p_OZ_wzKEQ"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('assets/spam-data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXcVi2QCzKER",
        "outputId": "176b5636-7400-43ca-9b79-82a252ae4111"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   label                                               text\n",
              " 0      0  Go until jurong point, crazy.. Available only ...\n",
              " 1      0                      Ok lar... Joking wif u oni...\n",
              " 2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              " 3      0  U dun say so early hor... U c already then say...\n",
              " 4      0  Nah I don't think he goes to usf, he lives aro...,\n",
              " (5572, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "dataset.head(), dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znpR2NggzKEU"
      },
      "source": [
        "## Extract dependent and independent features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "A7cmdmTdzKEV"
      },
      "outputs": [],
      "source": [
        "X = list(dataset['text'])\n",
        "y = list(dataset['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k14UFUD0zKEV"
      },
      "source": [
        "## Train-test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "sBeYrbtuzKEW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzNcmq9OzKEW"
      },
      "source": [
        "## Use a HuggingFace Model\n",
        "\n",
        "Generally, the steps involved in using a model from HuggingFace involves,\n",
        "1. Calling the pre-trained model\n",
        "2. Calling the model's tokenizer - since each model has it's own tokenizer\n",
        "3. Use the tokenizer to encode the train and test datasets\n",
        "   1. `truncation` - remove whitespace from each data point\n",
        "   2. `padding` - conform all data points to the same length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "7cATB22jzKEX"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "GfqBXCaFzKEY"
      },
      "outputs": [],
      "source": [
        "trainEncoded = tokenizer(X_train, truncation=True, padding=True)\n",
        "testEncoded = tokenizer(X_test, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "nxgogoBZzKEY"
      },
      "outputs": [],
      "source": [
        "# print(testEncoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpAJ_m3kzKEZ"
      },
      "source": [
        "## Create Dataset Objects with Tensorflow\n",
        "\n",
        "In tensorflow, the dataset objects are tensors. We do this so data flows through our pipeline in the expected format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Uq17Rqn7zKEa"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "trainDataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(trainEncoded),\n",
        "    y_train\n",
        "))\n",
        "\n",
        "testDataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(testEncoded),\n",
        "    y_test\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "brRe9x5ezKEb"
      },
      "outputs": [],
      "source": [
        "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
        "\n",
        "trainingArguments = TFTrainingArguments(\n",
        "    output_dir = '../results',\n",
        "    num_train_epochs = 2,\n",
        "    evaluation_strategy = 'steps',\n",
        "    eval_steps = 500,\n",
        "    per_device_train_batch_size = 4,\n",
        "    per_device_eval_batch_size = 8,\n",
        "    warmup_steps = 100,\n",
        "    weight_decay = 0.01,\n",
        "    logging_dir = '../logs',\n",
        "    logging_steps = 10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with trainingArguments.strategy.scope():\n",
        "    model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEupt2fqzsS_",
        "outputId": "0dec9c66-81d3-4220-ab9c-407f8cec75b8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = TFTrainer(\n",
        "    model = model,\n",
        "    args = trainingArguments,\n",
        "    train_dataset = trainDataset,\n",
        "    eval_dataset = testDataset\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3hOiNhR0G7V",
        "outputId": "4ba9930a-850d-4aa2-a6c5-6943ddeafb2f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/trainer_tf.py:118: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "P7sHru_30Y7v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "major-project-1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}