{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0xVolt/whats-up-doc/blob/main/src/experimental-notebooks/compare-models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UED-fV4iG5iu"
      },
      "source": [
        "# Compare Various HuggingFace Models for CDG by `SmoothedBLEU` Score\n",
        "\n",
        "## Purpose\n",
        "\n",
        "The purpose of this notebook is to figure out which models will work best for our CLI app. We will calculate and compare these select models by their smoothed BLEU scores and possibly look into fine-tuning them with [this dataset](https://www.dropbox.com/sh/488bq2of10r4wvw/AACs5CGIQuwtsD7j_Ls_JAORa/finetuning_dataset?dl=0&subfolder_nav_tracking=1).\n",
        "\n",
        "## Models\n",
        "\n",
        "These are the models we'll create inference points for and load in this notebook.\n",
        "1. [SEBIS/code_trans_t5_base_code_documentation_generation_python](https://huggingface.co/SEBIS/code_trans_t5_base_code_documentation_generation_python)\n",
        "2. [Salesforce/codet5-base-multi-sum](https://huggingface.co/Salesforce/codet5-base-multi-sum)\n",
        "3. [google/flan-t5-small](https://huggingface.co/google/flan-t5-small)\n",
        "\n",
        "The models listed above have been fine-tuned on the CodeSearchNet dataset across various programming languages for PL-NL sequence-to-sequence tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8E79z95HYnR"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wJbpYXdGluQ",
        "outputId": "9f9a1b40-ff83-4431-98d3-05fbc0927d70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q transformers sentencepiece datasets nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelWithLMHead, RobertaTokenizer, T5Tokenizer, T5ForConditionalGeneration, SummarizationPipeline\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define evaluation metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KjjG2VfSHXpA"
      },
      "outputs": [],
      "source": [
        "def calculateSmoothedBLEU(reference, candidate):\n",
        "    \"\"\"\n",
        "    Calculate the Smoothed BLEU-4 score between a reference string and a candidate string.\n",
        "\n",
        "    Args:\n",
        "    reference (str): The reference (ground truth) string.\n",
        "    candidate (str): The candidate (generated) string.\n",
        "\n",
        "    Returns:\n",
        "    float: The Smoothed BLEU-4 score.\n",
        "    \"\"\"\n",
        "    reference_tokens = reference.split()\n",
        "    candidate_tokens = candidate.split()\n",
        "\n",
        "    smoother = SmoothingFunction()\n",
        "    bleu_score = sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoother.method1)\n",
        "\n",
        "    return bleu_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\deshi\\anaconda3\\envs\\py39-torch\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1509: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Downloading (…)lve/main/config.json: 100%|██████████| 902/902 [00:00<00:00, 226kB/s]\n",
            "c:\\Users\\deshi\\anaconda3\\envs\\py39-torch\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\deshi\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Downloading pytorch_model.bin:   5%|▍         | 41.9M/892M [00:27<09:18, 1.52MB/s]"
          ]
        }
      ],
      "source": [
        "codeTransT5Model = AutoModelWithLMHead.from_pretrained(\"SEBIS/code_trans_t5_base_source_code_summarization_python_transfer_learning_finetune\")\n",
        "codeTransT5Tokenizer = AutoTokenizer.from_pretrained(\"SEBIS/code_trans_t5_base_source_code_summarization_python_transfer_learning_finetune\", skip_special_tokens=True)\n",
        "\n",
        "codeT5Model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base-multi-sum')\n",
        "codeT5Tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base-multi-sum', skip_special_tokens=True)\n",
        "\n",
        "flanT5Model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", device_map=\"auto\")\n",
        "flanT5Tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\", skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPTWE3sLB2iBAUp0BOuv8y3",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
